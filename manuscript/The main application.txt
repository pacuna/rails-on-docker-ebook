# The Main Application

Some extra configuration for this task definition consists in
environment variables that the container needs for connecting with all the other services. For example,
we need environment variables for passing the MySQL credentials and for all
the dependencies hostnames.

A quick way for getting all the endpoints for the internal ELBs is to run:

    $ aws elb describe-load-balancers --query "LoadBalancerDescriptions[*].DNSName"

Output:

    [
	"internal-mysql-elb-1700927984.us-east-1.elb.amazonaws.com",
	"internal-elasticseach-elb-1178069566.us-east-1.elb.amazonaws.com",
	"internal-elasticsearch-elb-907663077.us-east-1.elb.amazonaws.com",
	"internal-redis-elb-124984907.us-east-1.elb.amazonaws.com",
	"internal-memcached-elb-47598939.us-east-1.elb.amazonaws.com"
    ]

Let's use those endpoints in the application task definition:

    {
	"containerDefinitions": [
	    {
		"volumesFrom": [],
		"memory": 512,
		"portMappings": [
		    {
			"hostPort": 80,
			"containerPort": 80,
			"protocol": "tcp"
		    }
		],
		"essential": true,
		"entryPoint": [],
		"mountPoints": [],
		"name": "searchapp-webapp",
		"environment": [
		    {
			"name": "MEMCACHED_URL",
			"value": "internal-memcached-elb-47598939.us-east-1.elb.amazonaws.com"
		    },
		    {
			"name": "DB_PORT_80_TCP_ADDR",
			"value": "internal-mysql-elb-1700927984.us-east-1.elb.amazonaws.com"
		    },
		    {
			"name": "DB_ENV_MYSQL_PASSWORD",
			"value": "mysupersecretpassword"
		    },
		    {
			"name": "PASSENGER_APP_ENV",
			"value": "production"
		    },
		    {
			"name": "ELASTICSEARCH_URL",
			"value": "http://internal-elasticsearch-elb-907663077.us-east-1.elb.amazonaws.com"
		    },
		    {
			"name": "DB_ENV_MYSQL_NAME",
			"value": "searchapp_production"
		    },
		    {
			"name": "SECRET_KEY_BASE",
			"value": "6ada559ef69d8aa0501524f9ca8378122a5f25945d073e1c5aef0b88156fb9609b02fab9710b94343fef5f720f76a9619c54cbe93483749949fccce1b94e5fe0"
		    },
		    {
			"name": "DB_ENV_MYSQL_USER",
			"value": "root"
		    },
		    {
			"name": "REDIS_URL",
			"value": "internal-redis-elb-124984907.us-east-1.elb.amazonaws.com"
		    }
		],
		"links": [],
		"image": "username/searchapp:1.0",
		"command": [],
		"cpu": 512,
	    }
	],
	"volumes": [],
	"family": "searchapp"
    }

Now, of course all of this configuration is already in the respective files
of the application. For example, the MySQL configuration is in the `config/database.yml` file,
the elasticsearch configuration is in the `config/initializers/elasticsearch.rb`, the redis
configuration is in a sidekiq initializer and the memcached configuration is in the environment
file for the production environment.

As usual, we need an ELB for accessing our application. Let's create one now:

    $ aws elb create-load-balancer --load-balancer-name searchapp-elb --listeners Protocol=HTTP,LoadBalancerPort=80,InstanceProtocol=HTTP,InstancePort=80 --subnets $SubnetId --security-groups $GroupId

Output:

    {
	"DNSName": "searchapp-elb-1102698401.us-east-1.elb.amazonaws.com"
    }

I have configured an special endpoint that I want the health check for this ELB to use. This is because
once our application is up, we first need to index all the Elasticsearch data. But the ELB
will try to ping the default health check route which is '/' and that's going to return an error (the ELB needs a 200 status code), 
causing the task to be restarted. So instead of that route, we'll use a custom '/healthcheck' route which always
responds 200 as long as our application is up and will not return errors for missing indexes.

For configuring the health check endpoint let's run:

    $ aws elb configure-health-check --load-balancer-name searchapp-elb --health-check Target=HTTP:80/healthcheck,Interval=30,UnhealthyThreshold=2,HealthyThreshold=2,Timeout=3

Output:

    {
	"HealthCheck": {
	    "HealthyThreshold": 2,
	    "Interval": 30,
	    "Target": "HTTP:80/healthcheck",
	    "Timeout": 3,
	    "UnhealthyThreshold": 2
	}
    }

Another difference with this ELB is we don't want it to be an internal ELB. It has to be public
so people can access our application, therefore we don't need the internal flag.

We also have to expose the port 80 in our instances and in the ELB. All of these resources are sharing
the same security group, and the easiest way is to add a new rule for open access to the port 80 on that security group:

    $ aws ec2 authorize-security-group-ingress --group-id $GroupId --protocol tcp --port 80 --cidr 0.0.0.0/0

The json format for our service application is:

    {
	"serviceName": "searchapp-service",
	"taskDefinition": "searchapp:1",
	"loadBalancers": [
	    {
		"loadBalancerName": "searchapp-elb",
		"containerName": "searchapp-webapp",
		"containerPort": 80
	    }
	],
	"desiredCount": 1,
	"role": "ecsServiceRole"
    }

Let's first register our task definition:

    $ aws ecs register-task-definition --cli-input-json file://task-definitions/searchapp.json

Output:

    {
	"taskDefinition": {
	    "status": "ACTIVE",
	    "family": "searchapp",
	    "volumes": [],
	    "taskDefinitionArn": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp:1",
	    "containerDefinitions": [
		{
		    "environment": [
			{
			    "name": "MEMCACHED_URL",
			    "value": "internal-memcached-elb-47598939.us-east-1.elb.amazonaws.com"
			},
			{
			    "name": "DB_PORT_80_TCP_ADDR",
			    "value": "internal-mysql-elb-1700927984.us-east-1.elb.amazonaws.com"
			},
			{
			    "name": "DB_ENV_MYSQL_PASSWORD",
			    "value": "mysupersecretpassword"
			},
			{
			    "name": "PASSENGER_APP_ENV",
			    "value": "production"
			},
			{
			    "name": "ELASTICSEARCH_URL",
			    "value": "http://internal-elasticsearch-elb-907663077.us-east-1.elb.amazonaws.com"
			},
			{
			    "name": "DB_ENV_MYSQL_NAME",
			    "value": "searchapp_production"
			},
			{
			    "name": "SECRET_KEY_BASE",
			    "value": "6ada559ef69d8aa0501524f9ca8378122a5f25945d073e1c5aef0b88156fb9609b02fab9710b94343fef5f720f76a9619c54cbe93483749949fccce1b94e5fe0"
			},
			{
			    "name": "DB_ENV_MYSQL_USER",
			    "value": "root"
			},
			{
			    "name": "REDIS_URL",
			    "value": "internal-redis-elb-124984907.us-east-1.elb.amazonaws.com"
			}
		    ],
		    "name": "searchapp-webapp",
		    "links": [],
		    "mountPoints": [],
		    "image": "username/searchapp:1.0",
		    "essential": true,
		    "portMappings": [
			{
			    "protocol": "tcp",
			    "containerPort": 80,
			    "hostPort": 80
			}
		    ],
		    "entryPoint": [],
		    "memory": 512,
		    "command": [],
		    "cpu": 512,
		    "volumesFrom": []
		}
	    ],
	    "revision": 1
	}
    }

And create the service:

    $ aws ecs create-service --cluster searchapp --service-name searchapp-service --cli-input-json file://services/searchapp-service.json

Output:

    {
	"service": {
	    "status": "ACTIVE",
	    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp:1",
	    "pendingCount": 0,
	    "loadBalancers": [
		{
		    "containerName": "searchapp-webapp",
		    "containerPort": 80,
		    "loadBalancerName": "searchapp-elb"
		}
	    ],
	    "roleArn": "arn:aws:iam::387705308362:role/ecsServiceRole",
	    "desiredCount": 1,
	    "serviceName": "searchapp-service",
	    "clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp",
	    "serviceArn": "arn:aws:ecs:us-east-1:387705308362:service/searchapp-service",
	    "deployments": [
		{
		    "status": "PRIMARY",
		    "pendingCount": 0,
		    "createdAt": 1444502776.876,
		    "desiredCount": 1,
		    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp:1",
		    "updatedAt": 1444502776.876,
		    "id": "ecs-svc/9223370592351998931",
		    "runningCount": 0
		}
	    ],
	    "events": [],
	    "runningCount": 0
	}
    }

Wait for few minutes, and if everything went OK, visit the DNS address of the `searchapp-elb` and you'll see the
empty application. In my case the endpoint is `searchapp-elb-1102698401.us-east-1.elb.amazonaws.com`

![Empty Web Application](images/empty-app.png)

Great! If you can see the web application, it means the database connection and the migrations were
successful. We still don't have data, and that's because all the data for this sample application gets created through
database seeds. So the next step is to seed the database and then index the data.

We need to access our application in order to seed the data.
A quick way for getting the DNS of the instance that's running this tasks is by describing the associated
ELB and query directly for the instance id:

    $ aws elb describe-load-balancers --load-balancer-name searchapp-elb --query "LoadBalancerDescriptions[*].Instances[*].InstanceId"

Output:

    [
	[
	    "i-8372af57"
	]
    ]


And then we can describe the instance using the instance id and query for the DNS:

    $ aws ec2 describe-instances --instance-ids i-8372af57 --query "Reservations[*].Instances[*].PublicDnsName"

Output:

    [
	[
	    "ec2-52-23-234-174.compute-1.amazonaws.com"
	]
    ]

Cool, let's connect to that instance:

    $ ssh -i ~/.ssh/keys/ClusterKeyPair.pem ec2-user@ec2-52-23-234-174.compute-1.amazonaws.com

       __|  __|  __|
       _|  (   \__ \   Amazon ECS-Optimized Amazon Linux AMI 2015.03.g
     ____|\___|____/

    For documentation visit, http://aws.amazon.com/documentation/ecs
    No packages needed for security; 1 packages available
    Run "sudo yum update" to apply all updates.
    Amazon Linux version 2015.09 is available.
    -bash: warning: setlocale: LC_CTYPE: cannot change locale (UTF-8): No such file or directory
    [ec2-user@ip-172-31-1-131 ~]$

If you run docker ps on your instance, you'll see that the `searchapp` container is running:

    $ docker ps
    CONTAINER ID        IMAGE                            COMMAND                CREATED             STATUS              PORTS                         NAMES
    f3831182e38d        username/searchapp:1.0             "/sbin/my_init"        22 minutes ago      Up 22 minutes       0.0.0.0:80->80/tcp, 443/tcp   ecs-searchapp-1-searchapp-webapp-cccfafcefcb7f589a201
    efd0080d60b9        memcached                        "/entrypoint.sh memc   4 hours ago         Up 4 hours          0.0.0.0:11211->11211/tcp      ecs-memcached-1-searchapp-memcached-fc96dee99bf0828c2c00
    957d272d3081        amazon/amazon-ecs-agent:latest   "/agent"               16 hours ago        Up 16 hours         127.0.0.1:51678->51678/tcp    ecs-agent

We can execute commands in that container as usual. I prefer to go inside the container and run 
the commands there. We can do this by running (replace the container id with the one in your output):

    $ docker exec -it f3831182e38d bash

    root@f3831182e38d:/home/app/webapp#

Now we are inside of the rails application. You can run all the typical commands, rails console, rake tasks, tail
the log, etc. Just keep in mind that you're in production environment.

Now, before running the seeds, I've had some troubles with the database collation, but they are really easy to
fix. We just have to recreate our database, connect to it and run a MySQL command.

First let's recreate our database:

    $ RAILS_ENV=production rake db:drop
    $ RAILS_ENV=production rake db:create
    $ RAILS_ENV=production rake db:migrate

In this container we have MySQL available, so we can connect right from here. Look for your MySQL load balancer
DNS address (in your searchapp task definition or listing your ELBs):

    $ mysql -h internal-mysql-elb-1700927984.us-east-1.elb.amazonaws.com -u root -pmysupersecretpassword -P 80
    Welcome to the MySQL monitor.  Commands end with ; or \g.
    Your MySQL connection id is 1119
    Server version: 5.7.8-rc MySQL Community Server (GPL)

    Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.

    Oracle is a registered trademark of Oracle Corporation and/or its
    affiliates. Other names may be trademarks of their respective
    owners.

    Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

    mysql>

And in the MySQL console run:

    ALTER DATABASE searchapp_production CHARACTER SET utf8 COLLATE utf8_general_ci;


Output:

    Query OK, 1 row affected (0.00 sec)


And that's it. Type exit to get back to the container, and now run the seeds:

    $ RAILS_ENV=production rake db:seed

You'll see a lot of output of the data being inserted into the database. Just wait
until it finishes.

Once it's ready, we can index the data by using the task provided by the `elasticsearch-rails` gem
(you can use this task anytime you change your index or you want to reindex your data):

    $ RAILS_ENV=production bundle exec rake environment elasticsearch:import:model CLASS='Article' FORCE='y' BATCH=100

Output:

    [IMPORT] Done

Now if you visit the web page of the application again, you'll see that this time we have data:

![Complete Web Application](images/complete-app.png)

That's great. Right now we're sure that MySQL and Elasticsearch are working correctly.
But what about Redis and Memcached? Well, the project comes with a worker ready for use.
This worker will index the data using Sidekiq every time a record gets modified.

First let's run the worker:

    root@f3831182e38d:/home/app/webapp# RAILS_ENV=production bundle exec sidekiq --queue elasticsearch --verbose

The console will be attached while we have Sidekiq running. We just want to check if the hook with Redis is correct:

If you see the first line of the output in the console, you should see something like this:

    2015-10-10T19:36:16.169Z 1377 TID-a8c4g INFO: Booting Sidekiq 3.3.4 with redis options {:url=>"redis://internal-redis-elb-124984907.us-east-1.elb.amazonaws.com:80/12"}

Which means that the connection was successful. You can try opening another session inside the container
while you have Sidekiq running and modify some Article using the rails console. You should see some output
in the Sidekiq window for the record being reindexed.

Let's hit Ctrl-c for exiting Sidekiq and open a rails console:

    root@f3831182e38d:/home/app/webapp# rails c production

And let's clean the cache:

    irb(main):001:0> Rails.cache.clear
    Cache clear: flushing all keys
    Dalli::Server#connect internal-memcached-elb-47598939.us-east-1.elb.amazonaws.com:80
    => [true]

You can see the connection with memcached is working correctly.

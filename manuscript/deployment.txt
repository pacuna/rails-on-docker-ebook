# Deployment

## Introduction

## Creating the cluster

We can create the cluster using one simple command. The only parameter
that we need is the cluster name.

	$ aws ecs create-cluster --cluster-name "searchapp"

Output:

	{
	    "cluster": {
		"status": "ACTIVE",
		"clusterName": "searchapp",
		"registeredContainerInstancesCount": 0,
		"pendingTasksCount": 0,
		"runningTasksCount": 0,
		"activeServicesCount": 0,
		"clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp"
	    }
	}

The cluster was created successfully, it has zero EC2 container
instances registered, zero task running and no services.

The idea behind the ECS cluster is that you can forget about where and how you're
task and services will be deployed. You just have to launch them and Amazon
will take care of assigning the service or task to an instance. The only thing
that we have to take care for now is adding some EC2 instances to the cluster.

## Launching the EC2 instances

Now that we have our cluster, we can launch some instances and register them
in our cluster. For launching the instances we'll need:

- A Key Pair for ssh access
- A Security Group

Let's create our key pair and send the key to a pem file by querying the KeyMaterial
key from the output:

	$ aws ec2 create-key-pair --key-name ClusterKeyPair --query 'KeyMaterial' --output text > ClusterKeyPair.pem

Set the permissions of the file:

	$ chmod 400 ClusterKeyPair.pem

And move the file to your ssh keys directory or some place safe. Remember
with just this file you'll be able to ssh into your cluster instances.

For the security groups for the instances, we can use our default security group
for our default vpc. In order to get the information for our default vpc security
group, you can run:

	$ aws ec2 describe-security-groups

Output:

	{
	    "SecurityGroups": [
		{
		    "IpPermissionsEgress": [
			{
			    "IpProtocol": "-1",
			    "IpRanges": [
				{
				    "CidrIp": "0.0.0.0/0"
				}
			    ],
			    "UserIdGroupPairs": [],
			    "PrefixListIds": []
			}
		    ],
		    "Description": "default VPC security group",
		    "IpPermissions": [
			{
			    "IpProtocol": "-1",
			    "IpRanges": [],
			    "UserIdGroupPairs": [
				{
				    "UserId": "387705308362",
				    "GroupId": "sg-c70ff2a1"
				}
			    ],
			    "PrefixListIds": []
			}
		    ],
		    "GroupName": "default",
		    "VpcId": "vpc-120cd476",
		    "OwnerId": "387705308362",
		    "GroupId": "sg-c70ff2a1"
		}
	    ]
	}

As you can see from the output, my default vpc group has the id sg-c70ff2a1 and belongs
to the vpc with id vpc-120cd476. We will se some of this data during the next steps.
This security group allows all traffic between resources that belongs to the group.
The only thing that we are going to need is access to the port 22 for getting
ssh access to our instances. We'll do that later.


Our instances are going to need an IAM Role in order to allow certain actions
on the ECS infrastructure. Lucky for us, amazon has already defined the set
of policies that we need. So we just have to create the Role, attach the 
policy, and then add that role to an instance profile so we can add the
Role on launching time of our instances.

In the deploy folder of the project, you'll find an standard trust policy for
creating our role. Just copy the file and run this command replacing your
path to the file.

	$ aws iam create-role --role-name ecsInstanceRole --assume-role-policy-document file://policies/AmazonEC2ContainerServiceforEC2Role-Trust-Policy.json

Output:

	{
	    "Role": {
		"AssumeRolePolicyDocument": {
		    "Version": "2012-10-17",
		    "Statement": {
			"Action": "sts:AssumeRole",
			"Effect": "Allow",
			"Principal": {
			    "Service": "ec2.amazonaws.com"
			}
		    }
		},
		"RoleId": "AROAIIH2CKLXDXCQ3YS4Q",
		"CreateDate": "2015-10-08T23:01:52.285Z",
		"RoleName": "ecsInstanceRole",
		"Path": "/",
		"Arn": "arn:aws:iam::387705308362:role/ecsInstanceRole"
	    }
	}

Now, let's attach the policy to our new ecsInstanceRole role:

	$ aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role --role-name ecsInstanceRole

The final step is to create the instace profile and attach the Role to that profile:

	$ aws iam create-instance-profile --instance-profile-name ecsInstanceRole

Output:

	{
	    "InstanceProfile": {
		"InstanceProfileId": "AIPAI5CUZHG54Y7P6V5SG",
		"Roles": [],
		"CreateDate": "2015-10-08T23:04:44.535Z",
		"InstanceProfileName": "ecsInstanceRole",
		"Path": "/",
		"Arn": "arn:aws:iam::387705308362:instance-profile/ecsInstanceRole"
	    }
	}

And attaching...

	$ aws iam add-role-to-instance-profile --instance-profile-name ecsInstanceRole --role-name ecsInstanceRole


This command doesn't return any output.

Now we are ready to launch the instances for the cluster.

The AMI that we'll use is amzn-ami-2015.03.g-amazon-ecs-optimized with the id
ami-4fe4852a. This image comes ready and optimized for ECS.

We are going to launch 2 instances of type t2.micro and we are going to use
the keys and instance Role that were created previously, one of our subnets and
the security group for the default vpc.

For getting the available subnets, run:

	$ aws ec2 describe-subnets

Output:

	{
	    "Subnets": [
		{
		    "VpcId": "vpc-120cd476",
		    "CidrBlock": "172.31.0.0/20",
		    "MapPublicIpOnLaunch": true,
		    "DefaultForAz": true,
		    "State": "available",
		    "AvailabilityZone": "us-east-1d",
		    "SubnetId": "subnet-39706b4e",
		    "AvailableIpAddressCount": 4091
		},
		{
		    "VpcId": "vpc-120cd476",
		    "CidrBlock": "172.31.16.0/20",
		    "MapPublicIpOnLaunch": true,
		    "DefaultForAz": true,
		    "State": "available",
		    "AvailabilityZone": "us-east-1a",
		    "SubnetId": "subnet-fa724fa3",
		    "AvailableIpAddressCount": 4091
		},
		{
		    "VpcId": "vpc-120cd476",
		    "CidrBlock": "172.31.48.0/20",
		    "MapPublicIpOnLaunch": true,
		    "DefaultForAz": true,
		    "State": "available",
		    "AvailabilityZone": "us-east-1c",
		    "SubnetId": "subnet-4660426d",
		    "AvailableIpAddressCount": 4091
		},
		{
		    "VpcId": "vpc-120cd476",
		    "CidrBlock": "172.31.32.0/20",
		    "MapPublicIpOnLaunch": true,
		    "DefaultForAz": true,
		    "State": "available",
		    "AvailabilityZone": "us-east-1e",
		    "SubnetId": "subnet-477ea97a",
		    "AvailableIpAddressCount": 4091
		}
	    ]
	}

I have 4 available subnets, all in my default vpc. I'll choose the first
one for launching the instances.

In the following command replace the security-groups-ids with your default
vpc security group id, your subnet-id and the Arn of the instance profile which
you can find in the output of the create-instance-profile.

The user-data parameter allow us to add some initial configuration to the instances.
In our case we are passing the necessary configuration for registering the instances
in our searchapp cluster. The user-data.sh file looks like this (also in the deploy folder):

	#!/bin/bash
	echo ECS_CLUSTER=searchapp >> /etc/ecs/ecs.config

Now we can run the command for launching the instances:

	$ aws ec2 run-instances --image-id ami-4fe4852a --count 2 --instance-type t2.micro --key-name ClusterKeyPair --security-group-ids sg-c70ff2a1 --subnet-id subnet-39706b4e  --iam-instance-profile Arn=arn:aws:iam::387705308362:instance-profile/ecsInstanceRole --associate-public-ip-address --user-data file://user-data/user-data.sh

Output:

    {
	"OwnerId": "387705308362",
	"ReservationId": "r-202269dd",
	"Groups": [],
	"Instances": [
	    {
		"Monitoring": {
		    "State": "disabled"
		},
		"PublicDnsName": "",
		"RootDeviceType": "ebs",
		"State": {
		    "Code": 0,
		    "Name": "pending"
		},
		"EbsOptimized": false,
		"LaunchTime": "2015-10-10T02:13:50.000Z",
		"PrivateIpAddress": "172.31.1.132",
		"ProductCodes": [],
		"VpcId": "vpc-120cd476",
		"StateTransitionReason": "",
		"InstanceId": "i-8272af56",
		"ImageId": "ami-4fe4852a",
		"PrivateDnsName": "ip-172-31-1-132.ec2.internal",
		"KeyName": "ClusterKeyPair",
		"SecurityGroups": [
		    {
			"GroupName": "default",
			"GroupId": "sg-c70ff2a1"
		    }
		],
		"ClientToken": "",
		"SubnetId": "subnet-39706b4e",
		"InstanceType": "t2.micro",
		...

You can poll the status of the launched instances using the command

	$ aws ec2 describe-instances


After a few minutes, try running this command for polling the state of the cluster
and see if the instances were correctly registered:

	$ aws ecs describe-clusters --clusters searchapp

Output:

	{
	    "clusters": [
		{
		    "status": "ACTIVE",
		    "clusterName": "searchapp",
		    "registeredContainerInstancesCount": 2,
		    "pendingTasksCount": 0,
		    "runningTasksCount": 0,
		    "activeServicesCount": 0,
		    "clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp"
		}
	    ],
	    "failures": []
	}

Great, the output is telling me that the searchapp cluster has 2 registered instances.

## Our first task definition

### MySQL

The task definition will contain the information of the container that you
will run. You can create the task definition using json format, and the register 
the task in your cluster using the aws cli.
You can find all the task defitions used in this book in the deploy/task-definitions
directory of the source code repository.

    {
	"containerDefinitions": [
	    {
		"volumesFrom": [],
		"memory": 256,
		"portMappings": [
		    {
			"hostPort": 3306,
			"containerPort": 3306,
			"protocol": "tcp"
		    }
		],
		"essential": true,
		"entryPoint": [],
		"mountPoints": [
		    {
			"containerPath": "/var/lib/mysql",
			"sourceVolume": "mysql-data",
			"readOnly": false
		    }
		],
		"name": "searchapp-db",
		"environment": [
		    {
			"name": "MYSQL_DATABASE",
			"value": "searchapp_production"
		    },
		    {
			"name": "MYSQL_PASSWORD",
			"value": "mysecretpassword"
		    },
		    {
			"name": "MYSQL_ROOT_PASSWORD",
			"value": "mysupersecretpassword"
		    },
		    {
			"name": "MYSQL_USER",
			"value": "searchappusr"
		    }
		],
		"links": [],
		"image": "mysql:5.7",
		"command": [],
		"cpu": 128
	    }
	],
	"volumes": [
	    {
		"name": "mysql-data"
	    }
	],
	"family": "searchapp-db"
    }

This template might look complicated, but there's a lot of irrelevant information
there. Actually, if you use the ECS console for registering task definitions, you'll
see that most of the information can be ommited. 
If you have used docker-compose in the past, you'll see some similaties between
the structure of the definition. Here you also have port mapping, environment
variables, links with other containers, and volumes for persisting container data.
This task is mapping the port 3306 to the host, declaring the environment variables
that the mysql image requires, and creating a volume for mounting the /var/lib/mysql
directory. This approach will allow us to avoid data lost when the mysql container
is restarted. The problem is that we are now coupled to an particular instance, since
ECS doesn't sync the data between instances. This is still a dark area in the containers
world and it's why some people prefer to run only stateless application on containers
and use more typical approaches for their database resources.

The command for register this task in our cluster is:

	$ aws ecs register-task-definition --cli-input-json file://task-definitions/mysql.json

Output:

	{
	    "taskDefinition": {
		"status": "ACTIVE",
		"family": "searchapp-db",
		"volumes": [
		    {
			"host": {},
			"name": "mysql-data"
		    }
		],
		"taskDefinitionArn": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
		"containerDefinitions": [
		    {
			"environment": [
			    {
				"name": "MYSQL_DATABASE",
				"value": "searchapp_production"
			    },
			    {
				"name": "MYSQL_PASSWORD",
				"value": "mysecretpassword"
			    },
			    {
				"name": "MYSQL_ROOT_PASSWORD",
				"value": "mysupersecretpassword"
			    },
			    {
				"name": "MYSQL_USER",
				"value": "searchappusr"
			    }
			],
			"name": "searchapp-db",
			"links": [],
			"mountPoints": [
			    {
				"sourceVolume": "mysql-data",
				"readOnly": false,
				"containerPath": "/var/lib/mysql"
			    }
			],
			"image": "mysql:5.7",
			"essential": true,
			"portMappings": [
			    {
				"protocol": "tcp",
				"containerPort": 3306,
				"hostPort": 3306
			    }
			],
			"entryPoint": [],
			"memory": 256,
			"command": [],
			"cpu": 128,
			"volumesFrom": []
		    }
		],
		"revision": 1
	    }
	}

Great, our task was created successfully. Now every time we make changes
to this taks, a new revision will be created as long as we use the same
family value when registering the new json file.
Also, now we can run a service scheduler for running a task using this task definition.
The service will be in charge of allocating the task somewhere in the cluster and
restarting the task if the container fail for some reason.
Since the service associated with this task will deploy the container anywhere
in the cluster, we need some mechanism for getting some static value that will
point to the task, in this case the mysql server. The native way to accomplish
this in ECS is to create an Elastic Load Balancer and then when you define the service
you can add an option for attaching it to the ELB. This way, even is the service
allocates the task in a different instance between deploys, it will always be
attached to the ELB.

## Creating an ELB for our task

Let's create an ELB for the MySQL container. Remember to change the subnet and the
security group with your default or custom vpc information. This load balance
will be internal because we just want to use it inside of our vpc network:

    $ aws elb create-load-balancer --load-balancer-name mysql-elb --listeners Protocol=TCP,LoadBalancerPort=80,InstanceProtocol=TCP,InstancePort=3306 --subnets subnet-39706b4e --scheme internal --security-groups sg-c70ff2a1

Output:

    {
	"DNSName": "internal-mysql-elb-1700927984.us-east-1.elb.amazonaws.com"
    }

The output is telling us where we should point our database url configuration in our Rails application. This
ELB will be pointing to an EC2 instance, in which the container will be running and mapping
its 3306 to the 3306 port of the instance.

## Running the task

Let's run this task just for checking out if everything's ok. First we are going
to make sure that the mysql that we just registered is actually on our task definitions
list:

    $ aws ecs list-task-definitions

Output:

    {
	"taskDefinitionArns": [
	    "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1"
	]
    }

Great, so let's run a task using this task definition.
You can run the task without a service with the following command:

    $ aws ecs run-task --cluster searchapp --task-definition searchapp-db:1

Output:

    {
	"failures": [],
	"tasks": [
	    {
		"taskArn": "arn:aws:ecs:us-east-1:387705308362:task/99f26c75-b7f6-4f3c-a126-0d6b14684160",
		"overrides": {
		    "containerOverrides": [
			{
			    "name": "searchapp-db"
			}
		    ]
		},
		"lastStatus": "PENDING",
		"containerInstanceArn": "arn:aws:ecs:us-east-1:387705308362:container-instance/52b56eb8-2657-45cb-8cb3-5fdcd008f162",
		"clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp",
		"desiredStatus": "RUNNING",
		"taskDefinitionArn": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
		"containers": [
		    {
			"containerArn": "arn:aws:ecs:us-east-1:387705308362:container/9ee2fd87-e252-4dde-bea5-ab2ae4b1abe5",
			"taskArn": "arn:aws:ecs:us-east-1:387705308362:task/99f26c75-b7f6-4f3c-a126-0d6b14684160",
			"lastStatus": "PENDING",
			"name": "searchapp-db"
		    }
		]
	    }
	]
    }

For polling the status of the task, we are going to need its amazon arn identifier.
We can get the running tasks with

    $ aws ecs list-tasks --cluster searchapp

Output:

    {
	"taskArns": [
	    "arn:aws:ecs:us-east-1:387705308362:task/99f26c75-b7f6-4f3c-a126-0d6b14684160"
	]
    }

The output contains just the Arn of the only task that we have launched. So
let's get more information using that Arn:

    $ aws ecs describe-tasks --cluster searchapp --tasks arn:aws:ecs:us-east-1:387705308362:task/99f26c75-b7f6-4f3c-a126-0d6b14684160

Output:

    {
	"failures": [],
	"tasks": [
	    {
		"taskArn": "arn:aws:ecs:us-east-1:387705308362:task/99f26c75-b7f6-4f3c-a126-0d6b14684160",
		"overrides": {
		    "containerOverrides": [
			{
			    "name": "searchapp-db"
			}
		    ]
		},
		"lastStatus": "RUNNING",
		"containerInstanceArn": "arn:aws:ecs:us-east-1:387705308362:container-instance/52b56eb8-2657-45cb-8cb3-5fdcd008f162",
		"clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp",
		"desiredStatus": "RUNNING",
		"taskDefinitionArn": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
		"containers": [
		    {
			"containerArn": "arn:aws:ecs:us-east-1:387705308362:container/9ee2fd87-e252-4dde-bea5-ab2ae4b1abe5",
			"taskArn": "arn:aws:ecs:us-east-1:387705308362:task/99f26c75-b7f6-4f3c-a126-0d6b14684160",
			"name": "searchapp-db",
			"networkBindings": [
			    {
				"protocol": "tcp",
				"bindIP": "0.0.0.0",
				"containerPort": 3306,
				"hostPort": 3306
			    }
			],
			"lastStatus": "RUNNING",
			"exitCode": 0
		    }
		]
	    }
	]
    }

The output is giving us a full report about our tasks. We know that's running, so we can assume
that the docker image wall pulled and run correctly, we know the Arn of the
container Instance in which the task was launched,  and that the task is mapping the
port 3306 of the container, with the port 3306 of the EC2 instance.

Now, how can we know that the MySQL server is ready and waiting for connections?
Well, all these resources are being launched using the default vpc security group, 
which means that the resources have no restricted traffic access inside of the security
group. Let's ssh into our of our instances and see if we can connect to the MySQL
server.

Let's try with the same instance that's running the task. Let's get more info
about this instance using its Arn:

    $ aws ecs describe-container-instances --cluster searchapp --container-instances arn:aws:ecs:us-east-1:387705308362:container-instance/52b56eb8-2657-45cb-8cb3-5fdcd008f162

If look out the output, you'll find a section that contains the instance id:

    "ec2InstanceId": "i-8272af56"

With that id you can get the public ip address of the instance using the ec2 api:

    $ aws ec2 describe-instances --instance-ids i-8272af56

    {
	"Reservations": [
	    {
		"OwnerId": "387705308362",
		"ReservationId": "r-202269dd",
		"Groups": [],
		"Instances": [
		    {
			"Monitoring": {
			    "State": "disabled"
			},
			"PublicDnsName": "ec2-52-23-203-99.compute-1.amazonaws.com",
			"State": {
			    "Code": 16,
			    "Name": "running"
			},
			"EbsOptimized": false,
			"LaunchTime": "2015-10-10T02:13:50.000Z",
			"PublicIpAddress": "52.23.203.99",
			"PrivateIpAddress": "172.31.1.132",
    ...

In my case the public dns of the instance is ec2-52-23-203-99.compute-1.amazonaws.com.
If I try to connect to the MySQL server from my local machine, it will fail, since
my ip is not in the vpc range. But we can ssh into the machine and try from there.

In order to connect to the machine via ssh, we have to add a rule to the security group, 
so the 22 port can be accessed from our ip. Remember to change the group-id with yours
and the cird ip part with your local ip address.

    $ aws ec2 authorize-security-group-ingress --group-id sg-c70ff2a1 --protocol tcp --port 22 --cidr 190.101.5.80/24


Now you should be able to connect to the instance from your network using the pem and login with the
ec2-user:

    $ ssh -i ~/.ssh/keys/ClusterKeyPair.pem ec2-user@ec2-52-23-203-99.compute-1.amazonaws.com

And you'll ssh into the instance:


       __|  __|  __|
       _|  (   \__ \   Amazon ECS-Optimized Amazon Linux AMI 2015.03.g
     ____|\___|____/

    For documentation visit, http://aws.amazon.com/documentation/ecs
    No packages needed for security; 1 packages available
    Run "sudo yum update" to apply all updates.
    Amazon Linux version 2015.09 is available.
    -bash: warning: setlocale: LC_CTYPE: cannot change locale (UTF-8): No such file or directory
    [ec2-user@ip-172-31-1-132 ~]$

Great! now that we are inside of the machine, let's try listing the running
containers:

    $ docker ps

Output:

    CONTAINER ID        IMAGE                            COMMAND                CREATED             STATUS              PORTS                        NAMES
    fd8e52ad8b34        mysql:5.7                        "/entrypoint.sh mysq   6 minutes ago       Up 6 minutes        0.0.0.0:3306->3306/tcp       ecs-searchapp-db-1-searchapp-db-bc97e9d6f3bff4cba801
    aadddba8536d        amazon/amazon-ecs-agent:latest   "/agent"               13 minutes ago      Up 13 minutes       127.0.0.1:51678->51678/tcp   ecs-agent

So as expected, the mysql container is running, and also the amazon-ecs-agent.
Let's see if we can connect to the MySQL service from this machine. For that
we are going to have to install the MySQL client first:

    # change to root user
    $ sudo su
    $ yum install mysql 
    $ exit
    # back to ec2-user
    $ mysql -h ec2-52-23-203-99.compute-1.amazonaws.com -usearchappusr -pmysecretpassword

Output:

    Welcome to the MySQL monitor.  Commands end with ; or \g.
    Your MySQL connection id is 2
    Server version: 5.7.8-rc MySQL Community Server (GPL)

    Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.

    Oracle is a registered trademark of Oracle Corporation and/or its
    affiliates. Other names may be trademarks of their respective
    owners.

    Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

    mysql>


Nice! we were able to connect the MySQL server using the credentials that we
declared in the mysql task definition. The other instance registered in the cluster
is also in the vpc default security group, so it should be able to connect to this
server.

But, we want to run this task using a service scheduler and attach the service
to a load balancer, so let's stop this task using its Arn:

    $ aws ecs stop-task --cluster searchapp --task arn:aws:ecs:us-east-1:387705308362:task/99f26c75-b7f6-4f3c-a126-0d6b14684160

Output:

    {
	"task": {
	    "taskArn": "arn:aws:ecs:us-east-1:387705308362:task/99f26c75-b7f6-4f3c-a126-0d6b14684160",
	    "overrides": {
		"containerOverrides": [
		    {
			"name": "searchapp-db"
		    }
		]
	    },
	    "lastStatus": "RUNNING",
	    "containerInstanceArn": "arn:aws:ecs:us-east-1:387705308362:container-instance/52b56eb8-2657-45cb-8cb3-5fdcd008f162",
	    "clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp",
	    "desiredStatus": "STOPPED",
	    "taskDefinitionArn": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
	    "containers": [
		{
		    "containerArn": "arn:aws:ecs:us-east-1:387705308362:container/9ee2fd87-e252-4dde-bea5-ab2ae4b1abe5",
		    "taskArn": "arn:aws:ecs:us-east-1:387705308362:task/99f26c75-b7f6-4f3c-a126-0d6b14684160",
		    "name": "searchapp-db",
		    "networkBindings": [
			{
			    "protocol": "tcp",
			    "bindIP": "0.0.0.0",
			    "containerPort": 3306,
			    "hostPort": 3306
			}
		    ],
		    "lastStatus": "RUNNING",
		    "exitCode": 0
		}
	    ]
	}
    }

You can see from the output that the desired status for the task is now STOPPED.
If you run:

    $ aws ecs list-tasks --cluster searchapp

You'll see that there are no more tasks running:

    {
	"taskArns": []
    }


## Our first simple service

Now that we know that our task definition works properly, let's create
a service scheduler for managing this task. 
This service will be connected to the ELB that we created previously.

If you list your current services you'll see an empty response:

    $ aws ecs list-services --cluster searchapp

Output:

    {
	"serviceArns": []
    }

Let's create one for the MySQL task:

    $ aws ecs create-service --cluster searchapp --service-name mysql-service --task-definition searchapp-db:1 --desired-count 1

Output:

    {
	"service": {
	    "status": "ACTIVE",
	    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
	    "pendingCount": 0,
	    "loadBalancers": [],
	    "desiredCount": 1,
	    "serviceName": "mysql-service",
	    "clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp",
	    "serviceArn": "arn:aws:ecs:us-east-1:387705308362:service/mysql-service",
	    "deployments": [
		{
		    "status": "PRIMARY",
		    "pendingCount": 0,
		    "createdAt": 1444447207.569,
		    "desiredCount": 1,
		    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
		    "updatedAt": 1444447207.569,
		    "id": "ecs-svc/9223370592407568238",
		    "runningCount": 0
		}
	    ],
	    "events": [],
	    "runningCount": 0
	}
    }

The create-service command just needs the cluster name, the service name, the
task definition for running the task, and the desired count of tasks. If you
choose more than 1 running tasks, the task will be allocated in more than one
instance, which in our case make sense, since the task is using a fixed port
on the instance (3306), so it's not possible to have more than one of this task
running on one instace.
You can let ECS to pick a random port for your task, but then you would no be
able to create a service attached to an ELB, since the ELB needs a static port
for its listener.

If you list your services again, you'll get the Arn for the service recently
created:

    $ aws ecs list-services --cluster searchapp

    {
	"serviceArns": [
	    "arn:aws:ecs:us-east-1:387705308362:service/mysql-service"
	]
    }

And you can run the describe-services command for get all the information for
a service using its Arn identifier:

    $ aws ecs describe-services --cluster searchapp --services arn:aws:ecs:us-east-1:387705308362:service/mysql-service

    {
	"services": [
	    {
		"status": "ACTIVE",
		"taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
		"pendingCount": 0,
		"loadBalancers": [],
		"desiredCount": 1,
		"serviceName": "mysql-service",
		"clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp",
		"serviceArn": "arn:aws:ecs:us-east-1:387705308362:service/mysql-service",
		"deployments": [
		    {
			"status": "PRIMARY",
			"pendingCount": 0,
			"createdAt": 1444447207.569,
			"desiredCount": 1,
			"taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
			"updatedAt": 1444447207.569,
			"id": "ecs-svc/9223370592407568238",
			"runningCount": 1
		    }
		],
		"events": [
		    {
			"message": "(service mysql-service) has reached a steady state.",
			"id": "13307911-f01a-44aa-bf79-b71ed2a9ca12",
			"createdAt": 1444447226.492
		    },
		    {
			"message": "(service mysql-service) has started 1 tasks: (task 8e710989-3ae2-40c5-a60b-258c12fbff47).",
			"id": "08814d81-e268-4b5e-ad31-3c9cdd9658ab",
			"createdAt": 1444447222.411
		    }
		],
		"runningCount": 1
	    }
	],
	"failures": []
    }

It's good to be familiarized with this kind of output, since later it's going to be
very useful for troubleshooting issues with the service. The events section
of the output tells us that the current state of the service and can show
possible errors during the launch. Later we'll talk more about debugging issues.
In our case, the service has started and has reached a steady state, which means
there's no issues.

So this is the most basic kind of service. Right now we need our service to
be attached to the ELB for the MySQL server. For removing a service, first
we have to update the desired count of tasks to zero:

    $ aws ecs update-service --cluster searchapp --service mysql-service --desired-count 0

Output

    {
	"service": {
	    "status": "ACTIVE",
	    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
	    "pendingCount": 0,
	    "loadBalancers": [],
	    "desiredCount": 0,
	    "serviceName": "mysql-service",
	    "clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp",
	    "serviceArn": "arn:aws:ecs:us-east-1:387705308362:service/mysql-service",
	    "deployments": [
		{
		    "status": "PRIMARY",
		    "pendingCount": 0,
		    "createdAt": 1444447207.569,
		    "desiredCount": 0,
		    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
		    "updatedAt": 1444447207.569,
		    "id": "ecs-svc/9223370592407568238",
		    "runningCount": 1
		}
	    ],
	    "events": [
		{
		    "message": "(service mysql-service) has reached a steady state.",
		    "id": "13307911-f01a-44aa-bf79-b71ed2a9ca12",
		    "createdAt": 1444447226.492
		},
		{
		    "message": "(service mysql-service) has started 1 tasks: (task 8e710989-3ae2-40c5-a60b-258c12fbff47).",
		    "id": "08814d81-e268-4b5e-ad31-3c9cdd9658ab",
		    "createdAt": 1444447222.411
		}
	    ],
	    "runningCount": 1
	}
    }

After a minute, there will be no tasks running in the cluster. Let's delete the service:

    $ aws ecs delete-service --cluster searchapp --service mysql-service

Output:

    {
	"service": {
	    "status": "DRAINING",
	    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
	    "pendingCount": 0,
	    "loadBalancers": [],
	    "desiredCount": 0,
	    "serviceName": "mysql-service",
	    "clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp",
	    "serviceArn": "arn:aws:ecs:us-east-1:387705308362:service/mysql-service",
	    "deployments": [
		{
		    "status": "PRIMARY",
		    "pendingCount": 0,
		    "createdAt": 1444447207.569,
		    "desiredCount": 0,
		    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
		    "updatedAt": 1444447949.667,
		    "id": "ecs-svc/9223370592407568238",
		    "runningCount": 0
		}
	    ],
	    "events": [
		{
		    "message": "(service mysql-service) has reached a steady state.",
		    "id": "258def28-8811-4b4d-a007-e15faba24618",
		    "createdAt": 1444447968.859
		},
		{
		    "message": "(service mysql-service) stopped 1 running tasks.",
		    "id": "09237224-5800-4a96-ad48-fc078e216cde",
		    "createdAt": 1444447965.169
		},
		{
		    "message": "(service mysql-service) has reached a steady state.",
		    "id": "13307911-f01a-44aa-bf79-b71ed2a9ca12",
		    "createdAt": 1444447226.492
		},
		{
		    "message": "(service mysql-service) has started 1 tasks: (task 8e710989-3ae2-40c5-a60b-258c12fbff47).",
		    "id": "08814d81-e268-4b5e-ad31-3c9cdd9658ab",
		    "createdAt": 1444447222.411
		}
	    ],
	    "runningCount": 0
	}
    }

The status of the service has changed to draining, which means that is going
to dissapear from the cluster.

## Our first more complex service

The simple service created in the previous section was enough for catching
the essentials of the workflow in ECS. You create your task definition, create
a service that runs that task, when you don't need the task running anymore you
update the desired-count of the service, and finally you can remove the service.

Now, let's create a service connected to the ELB. For this case we are going to
need a json file with the structure of the service. This is a good thing since now
we can add our task definitions and our services definitions to our version control system.
We're also going to need a new IAM Role, since this service requires more interaction
with our resources, specially with the instances and ELBs. We can later use this
same Role for the other services that we need.

Use the file deploy/AmazonEC2ContainerServiceRole-Trust-Policy.json for the trust
policy for this role:

    $ aws iam create-role --role-name ecsServiceRole --assume-role-policy-document file://policies/AmazonEC2ContainerServiceRole-Trust-Policy.json

Output:

    {
	"Role": {
	    "AssumeRolePolicyDocument": {
		"Version": "2012-10-17",
		"Statement": {
		    "Action": "sts:AssumeRole",
		    "Effect": "Allow",
		    "Principal": {
			"Service": "ecs.amazonaws.com"
		    }
		}
	    },
	    "RoleId": "AROAI4JOLLGSAZ7PJWZKA",
	    "CreateDate": "2015-10-10T03:49:23.138Z",
	    "RoleName": "ecsServiceRole",
	    "Path": "/",
	    "Arn": "arn:aws:iam::387705308362:role/ecsServiceRole"
	}
    }

And we need to attach the AmazonEC2ContainerServiceRole policy to this Role:

    $ aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceRole --role-name ecsServiceRole

Now that we have the Role, the json for the service should look like this (deploy/services/mysql-service.json):

    {
	"serviceName": "mysql-service",
	    "taskDefinition": "searchapp-db:1",
	    "loadBalancers": [
	    {
		"loadBalancerName": "mysql-elb",
		"containerName": "searchapp-db",
		"containerPort": 3306 
	    }
	    ],
		"desiredCount": 1,
		"role": "ecsServiceRole"
    }

And we create the service passing this file to the command (make sure you have the right path
for the json file):

    $ aws ecs create-service --cluster searchapp --service-name mysql-service --cli-input-json file://mysql-service.json

Output:

    {
	"service": {
	    "status": "ACTIVE",
	    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
	    "pendingCount": 0,
	    "loadBalancers": [
		{
		    "containerName": "searchapp-db",
		    "containerPort": 3306,
		    "loadBalancerName": "mysql-elb"
		}
	    ],
	    "roleArn": "arn:aws:iam::387705308362:role/ecsServiceRole",
	    "desiredCount": 1,
	    "serviceName": "mysql-service",
	    "clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp",
	    "serviceArn": "arn:aws:ecs:us-east-1:387705308362:service/mysql-service",
	    "deployments": [
		{
		    "status": "PRIMARY",
		    "pendingCount": 0,
		    "createdAt": 1444451891.317,
		    "desiredCount": 1,
		    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
		    "updatedAt": 1444451891.317,
		    "id": "ecs-svc/9223370592402884475",
		    "runningCount": 0
		},
		{
		    "status": "ACTIVE",
		    "pendingCount": 0,
		    "createdAt": 1444449447.38,
		    "desiredCount": 0,
		    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
		    "updatedAt": 1444451661.788,
		    "id": "ecs-svc/9223370592405328421",
		    "runningCount": 0
		}
	    ],
	    "events": [],
	    "runningCount": 0
	}
    }

If we wait a minute and describe the service using its Arn in the events section we'll get:

    $ aws ecs describe-services --cluster searchapp --services arn:aws:ecs:us-east-1:387705308362:service/mysql-service


    {
	"services": [
	    {
		"status": "ACTIVE",
		"taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
		"pendingCount": 0,
		"loadBalancers": [
		    {
			"containerName": "searchapp-db",
			"containerPort": 3306,
			"loadBalancerName": "mysql-elb"
		    }
		],
		"roleArn": "arn:aws:iam::387705308362:role/ecsServiceRole",
		"desiredCount": 1,
		"serviceName": "mysql-service",
		"clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp",
		"serviceArn": "arn:aws:ecs:us-east-1:387705308362:service/mysql-service",
		"deployments": [
		    {
			"status": "PRIMARY",
			"pendingCount": 0,
			"createdAt": 1444451891.317,
			"desiredCount": 1,
			"taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
			"updatedAt": 1444451891.317,
			"id": "ecs-svc/9223370592402884475",
			"runningCount": 1
		    }
		],
		"events": [
		    {
			"message": "(service mysql-service) registered 1 instances in (elb mysql-elb)",
			"id": "171d065e-80ef-4aef-ab59-f8da4e3eab8a",
			"createdAt": 1444451918.439
		    },
		    {
			"message": "(service mysql-service) deregistered 1 instances in (elb mysql-elb)",
			"id": "36b9bcee-d80c-476d-bc5e-e396201ca38e",
			"createdAt": 1444451912.185
		    },
		    {
			"message": "(service mysql-service) has started 1 tasks: (task 78120397-b542-44ce-903c-4b14549a7314).",
			"id": "465fc2bf-5a4a-4965-b490-c0ddd2316467",
			"createdAt": 1444451912.185
		    }
		],
		"runningCount": 1
	    }
	],
	"failures": []
    }

Now there's an event with the message "(service mysql-service) registered 1 instances in (elb mysql-elb)"
which means that our MySQL service should now be available from within the url
of the elb. Let's get the ELB url and then try to connect to the MySQL service from inside
of one of our instances:

    $ aws elb describe-load-balancers

Output:

    {
	"LoadBalancerDescriptions": [
	    {
		"Subnets": [
		    "subnet-39706b4e"
		],
		"CanonicalHostedZoneNameID": "Z3DZXE0Q79N41H",
		"VPCId": "vpc-120cd476",
		"ListenerDescriptions": [
		    {
			"Listener": {
			    "InstancePort": 3306,
			    "LoadBalancerPort": 80,
			    "Protocol": "TCP",
			    "InstanceProtocol": "TCP"
			},
			"PolicyNames": []
		    }
		],
		"HealthCheck": {
		    "HealthyThreshold": 10,
		    "Interval": 30,
		    "Target": "TCP:3306",
		    "Timeout": 5,
		    "UnhealthyThreshold": 2
		},
		"BackendServerDescriptions": [],
		"Instances": [
		    {
			"InstanceId": "i-8372af57"
		    }
		],
		"DNSName": "internal-mysql-elb-1700927984.us-east-1.elb.amazonaws.com",
		"SecurityGroups": [
		    "sg-c70ff2a1"
		],
		"Policies": {
		    "LBCookieStickinessPolicies": [],
		    "AppCookieStickinessPolicies": [],
		    "OtherPolicies": []
		},
		"LoadBalancerName": "mysql-elb",
		"CreatedTime": "2015-10-10T04:36:34.930Z",
		"AvailabilityZones": [
		    "us-east-1d"
		],
		"Scheme": "internal",
		"SourceSecurityGroup": {
		    "OwnerAlias": "387705308362",
		    "GroupName": "default"
		}
	    }
	]
    }

As you can see from this output, the dns for this load balancer is internal-mysql-elb-1700927984.us-east-1.elb.amazonaws.com
and it has one instance attached.

Let's connect to the same instance as before but this time let's try using the ELB dns
for connecting to the MySQL server, this time using the port 80:

    $ ssh -i ~/.ssh/keys/ClusterKeyPair.pem ec2-user@ec2-52-23-203-99.compute-1.amazonaws.com

       __|  __|  __|
       _|  (   \__ \   Amazon ECS-Optimized Amazon Linux AMI 2015.03.g
     ____|\___|____/

    For documentation visit, http://aws.amazon.com/documentation/ecs
    No packages needed for security; 1 packages available
    Run "sudo yum update" to apply all updates.
    Amazon Linux version 2015.09 is available.
    -bash: warning: setlocale: LC_CTYPE: cannot change locale (UTF-8): No such file or directory

    $ mysql -h internal-mysql-elb-1700927984.us-east-1.elb.amazonaws.com -P 80 -usearchappusr -pmysecretpassword

    Welcome to the MySQL monitor.  Commands end with ; or \g.
    Your MySQL connection id is 7
    Server version: 5.7.8-rc MySQL Community Server (GPL)

    Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.

    Oracle is a registered trademark of Oracle Corporation and/or its
    affiliates. Other names may be trademarks of their respective
    owners.

    Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

    mysql>

Great! so we have our first service in place using our internal load balancer.
For the rest of the tasks, the setup will be pretty similar, we have to register
the task definitions, create a load balancer for each one and launch services
attached to this load balancer. Once we have all of our dependencies running
in their services, we can launch our main application and connect it to this
services.

## More task definitions

Now that we got the basics of the ECS workflow, let's register the task definitions
for the remaning dependencies: elasticsearch, redis and memcached. All of this
services needs to run behind a ELB so our main app can discover the endpoints
correctly.


### Elasticsearch

The task definitions for elasticsearch is:

    {
	"containerDefinitions": [
	    {
		"volumesFrom": [],
		"memory": 256,
		"portMappings": [
		    {
			"hostPort": 9200,
			"containerPort": 9200,
			"protocol": "tcp"
		    }
		],
		"essential": true,
		"entryPoint": [],
		"mountPoints": [
		    {
			"containerPath": "/usr/share/elasticsearch/data",
			"sourceVolume": "elasticsearch-data",
			"readOnly": false
		    }
		],
		"name": "searchapp-es",
		"environment": [],
		"links": [],
		"image": "elasticsearch",
		"command": [],
		"cpu": 128
	    }
	],
	"volumes": [
	    {
		"name": "elasticsearch-data"
	    }
	],
	"family": "elasticsearch"
    }

Pretty much the same as the MySQL task. We are using a volume for persisting the elasticsearc data, using 256 MiB
for memory, and 128 cpu units. The port mapping is 9200 from the container to 9200 on the instance. Let's register
this task on our cluster:

    $ aws ecs register-task-definition --cli-input-json file://task-definitions/elasticsearch.json

Output:

    {
	"taskDefinition": {
	    "status": "ACTIVE",
	    "family": "elasticsearch",
	    "volumes": [
		{
		    "host": {},
		    "name": "elasticsearch-data"
		}
	    ],
	    "taskDefinitionArn": "arn:aws:ecs:us-east-1:387705308362:task-definition/elasticsearch:1",
	    "containerDefinitions": [
		{
		    "environment": [],
		    "name": "searchapp-es",
		    "links": [],
		    "mountPoints": [
			{
			    "sourceVolume": "elasticsearch-data",
			    "readOnly": false,
			    "containerPath": "/usr/share/elasticsearch/data"
			}
		    ],
		    "image": "elasticsearch",
		    "essential": true,
		    "portMappings": [
			{
			    "protocol": "tcp",
			    "containerPort": 9200,
			    "hostPort": 9200
			}
		    ],
		    "entryPoint": [],
		    "memory": 256,
		    "command": [],
		    "cpu": 128,
		    "volumesFrom": []
		}
	    ],
	    "revision": 1
	}
    }

Now before we create the service let's add a new ELB for elasticsearch (remember to use your security group and subnet):


    $ aws elb create-load-balancer --load-balancer-name elasticsearch-elb --listeners Protocol=TCP,LoadBalancerPort=80,InstanceProtocol=TCP,InstancePort=9200 --subnets subnet-39706b4e --scheme internal --security-groups sg-c70ff2a1

Output:

    {
	"DNSName": "internal-elasticsearch-elb-907663077.us-east-1.elb.amazonaws.com"
    }

In this case we need to listen to the port 9200 on the instance.

Now we can launch the service using the task definition and the load balancer. The json file for this service is:

    {
	"serviceName": "elasticsearch-service",
	"taskDefinition": "elasticsearch:1",
	"loadBalancers": [
	    {
		"loadBalancerName": "elasticsearch-elb",
		"containerName": "searchapp-es",
		"containerPort": 9200
	    }
	],
	"desiredCount": 1,
	"role": "ecsServiceRole"
    }


And we launched with:

    $ aws ecs create-service --cluster searchapp --service-name elasticsearch-service --cli-input-json file://elasticsearch-service.json

Output:

    {
	"service": {
	    "status": "ACTIVE",
	    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/elasticsearch:1",
	    "pendingCount": 0,
	    "loadBalancers": [
		{
		    "containerName": "searchapp-es",
		    "containerPort": 9200,
		    "loadBalancerName": "elasticsearch-elb"
		}
	    ],
	    "roleArn": "arn:aws:iam::387705308362:role/ecsServiceRole",
	    "desiredCount": 1,
	    "serviceName": "elasticsearch-service",
	    "clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp",
	    "serviceArn": "arn:aws:ecs:us-east-1:387705308362:service/elasticsearch-service",
	    "deployments": [
		{
		    "status": "PRIMARY",
		    "pendingCount": 0,
		    "createdAt": 1444488166.315,
		    "desiredCount": 1,
		    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/elasticsearch:1",
		    "updatedAt": 1444488166.315,
		    "id": "ecs-svc/9223370592366609492",
		    "runningCount": 0
		}
	    ],
	    "events": [],
	    "runningCount": 0
	}
    }

If we follow the same procedure as before, discover in which instance the service is running and we try to reach
the service, we get:

    [ec2-user@ip-172-31-1-132 ~]$ curl localhost:9200
    {
      "status" : 200,
      "name" : "Amanda Sefton",
      "cluster_name" : "elasticsearch",
      "version" : {
	"number" : "1.7.2",
	"build_hash" : "e43676b1385b8125d647f593f7202acbd816e8ec",
	"build_timestamp" : "2015-09-14T09:49:53Z",
	"build_snapshot" : false,
	"lucene_version" : "4.10.4"
      },
      "tagline" : "You Know, for Search"
    }

Great, so we have that service ready.

## Redis

The task definition for redis is:


    {
	"containerDefinitions": [
	    {
		"volumesFrom": [],
		"memory": 128,
		"portMappings": [
		    {
			"hostPort": 6379,
			"containerPort": 6379,
			"protocol": "tcp"
		    }
		],
		"essential": true,
		"entryPoint": [],
		"mountPoints": [],
		"name": "searchapp-redis",
		"environment": [],
		"links": [],
		"image": "redis",
		"command": [],
		"cpu": 128
	    }
	],
	"volumes": [],
	"family": "redis"
    }

In this case we're not mounting a volume and we are using less memory than the first two.
The amount of memory and cpu units that you want to use for a task, it's probably going to be 
the outcome of experimentation with your containers. Sometimes you'll launch a task with a less
amount than the container really needs and the container process will be killed. So keep in mind
that a typical failure can be produced for using the wrong amount of memory and/our cpu units.

Let's register this task:

    $ aws ecs register-task-definition --cli-input-json file://task-definitions/redis.json

Output:

    {
	"taskDefinition": {
	    "status": "ACTIVE",
	    "family": "redis",
	    "volumes": [],
	    "taskDefinitionArn": "arn:aws:ecs:us-east-1:387705308362:task-definition/redis:1",
	    "containerDefinitions": [
		{
		    "environment": [],
		    "name": "searchapp-redis",
		    "links": [],
		    "mountPoints": [],
		    "image": "redis",
		    "essential": true,
		    "portMappings": [
			{
			    "protocol": "tcp",
			    "containerPort": 6379,
			    "hostPort": 6379
			}
		    ],
		    "entryPoint": [],
		    "memory": 128,
		    "command": [],
		    "cpu": 128,
		    "volumesFrom": []
		}
	    ],
	    "revision": 1
	}
    }

Create the ELB:

    $ aws elb create-load-balancer --load-balancer-name redis-elb --listeners Protocol=TCP,LoadBalancerPort=80,InstanceProtocol=TCP,InstancePort=6379 --subnets subnet-39706b4e --scheme internal --security-groups sg-c70ff2a1

Output:

    {
	"DNSName": "internal-redis-elb-124984907.us-east-1.elb.amazonaws.com"
    }

And launch the service using this json:


    {
	"serviceName": "redis-service",
	"taskDefinition": "searchapp-redis:1",
	"loadBalancers": [
	    {
		"loadBalancerName": "redis-elb",
		"containerName": "searchapp-redis",
		"containerPort": 6379
	    }
	],
	"desiredCount": 1,
	"role": "ecsServiceRole"
    }

Launch the service:

    $ aws ecs create-service --cluster searchapp --service-name redis-service --cli-input-json file://redis-service.json

Output:

    {
	"service": {
	    "status": "ACTIVE",
	    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/redis:1",
	    "pendingCount": 0,
	    "loadBalancers": [
		{
		    "containerName": "searchapp-redis",
		    "containerPort": 6379,
		    "loadBalancerName": "redis-elb"
		}
	    ],
	    "roleArn": "arn:aws:iam::387705308362:role/ecsServiceRole",
	    "desiredCount": 1,
	    "serviceName": "redis-service",
	    "clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp",
	    "serviceArn": "arn:aws:ecs:us-east-1:387705308362:service/redis-service",
	    "deployments": [
		{
		    "status": "PRIMARY",
		    "pendingCount": 0,
		    "createdAt": 1444489036.569,
		    "desiredCount": 1,
		    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/redis:1",
		    "updatedAt": 1444489036.569,
		    "id": "ecs-svc/9223370592365739238",
		    "runningCount": 0
		}
	    ],
	    "events": [],
	    "runningCount": 0
	}
    }

Great, we just one more dependency service.

## Memcached

The task definition is pretty similar to the redis one:


    {
	"containerDefinitions": [
	    {
		"volumesFrom": [],
		"memory": 128,
		"portMappings": [
		    {
			"hostPort": 11211,
			"containerPort": 11211,
			"protocol": "tcp"
		    }
		],
		"essential": true,
		"entryPoint": [],
		"mountPoints": [],
		"name": "searchapp-memcached",
		"environment": [],
		"links": [],
		"image": "memcached",
		"command": [],
		"cpu": 128
	    }
	],
	"volumes": [],
	"family": "memcached"
    }

Register the task:

    $ aws ecs register-task-definition --cli-input-json file://memcached.json

Output:

    {
	"taskDefinition": {
	    "status": "ACTIVE",
	    "family": "memcached",
	    "volumes": [],
	    "taskDefinitionArn": "arn:aws:ecs:us-east-1:387705308362:task-definition/memcached:1",
	    "containerDefinitions": [
		{
		    "environment": [],
		    "name": "searchapp-memcached",
		    "links": [],
		    "mountPoints": [],
		    "image": "memcached",
		    "essential": true,
		    "portMappings": [
			{
			    "protocol": "tcp",
			    "containerPort": 11211,
			    "hostPort": 11211
			}
		    ],
		    "entryPoint": [],
		    "memory": 128,
		    "command": [],
		    "cpu": 128,
		    "volumesFrom": []
		}
	    ],
	    "revision": 1
	}
    }

Create the ELB:

    $ aws elb create-load-balancer --load-balancer-name memcached-elb --listeners Protocol=TCP,LoadBalancerPort=80,InstanceProtocol=TCP,InstancePort=11211 --subnets subnet-39706b4e --scheme internal --security-groups sg-c70ff2a1

Output:

    {
	"DNSName": "internal-memcached-elb-47598939.us-east-1.elb.amazonaws.com"
    }

And launch a service using this json:


    {
	"serviceName": "memcached-service",
	"taskDefinition": "memcached:1",
	"loadBalancers": [
	    {
		"loadBalancerName": "memcached-elb",
		"containerName": "searchapp-memcached",
		"containerPort": 11211
	    }
	],
	"desiredCount": 1,
	"role": "ecsServiceRole"
    }

Create the service:

    $ aws ecs create-service --cluster searchapp --service-name memcached-service --cli-input-json file://memcached-service.json

Output:

    {
	"service": {
	    "status": "ACTIVE",
	    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/memcached:1",
	    "pendingCount": 0,
	    "loadBalancers": [
		{
		    "containerName": "searchapp-memcached",
		    "containerPort": 11211,
		    "loadBalancerName": "memcached-elb"
		}
	    ],
	    "roleArn": "arn:aws:iam::387705308362:role/ecsServiceRole",
	    "desiredCount": 1,
	    "serviceName": "memcached-service",
	    "clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp",
	    "serviceArn": "arn:aws:ecs:us-east-1:387705308362:service/memcached-service",
	    "deployments": [
		{
		    "status": "PRIMARY",
		    "pendingCount": 0,
		    "createdAt": 1444489403.347,
		    "desiredCount": 1,
		    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/memcached:1",
		    "updatedAt": 1444489403.347,
		    "id": "ecs-svc/9223370592365372460",
		    "runningCount": 0
		}
	    ],
	    "events": [],
	    "runningCount": 0
	}
    }

And that's it for services for our dependencies. If you list your services you'll see the four service that
we created:

    $ aws ecs list-services --cluster searchapp

Output:

    {
	"serviceArns": [
	    "arn:aws:ecs:us-east-1:387705308362:service/memcached-service",
	    "arn:aws:ecs:us-east-1:387705308362:service/elasticsearch-service",
	    "arn:aws:ecs:us-east-1:387705308362:service/mysql-service",
	    "arn:aws:ecs:us-east-1:387705308362:service/redis-service"
	]
    }

Now we star with out Rails application that's going to connect with each one of this services using the
internal ELBs that point to the services endpoints.

## Searchapp task definition

The only extra attributes that the task definition for our web application is going to have, are several
environment variables that the container needs for connecting with all the other services. So for example
we need environment variables for the MySQL credentials, and for the endpoints for elasticsearch, redis
and memcached. 

A quick way of getting all the endpoints for the internal ELB's is to run:

    $ aws elb describe-load-balancers --query "LoadBalancerDescriptions[*].DNSName"

Output:

    [
	"internal-mysql-elb-1700927984.us-east-1.elb.amazonaws.com",
	"internal-elasticseach-elb-1178069566.us-east-1.elb.amazonaws.com",
	"internal-elasticsearch-elb-907663077.us-east-1.elb.amazonaws.com",
	"internal-redis-elb-124984907.us-east-1.elb.amazonaws.com",
	"internal-memcached-elb-47598939.us-east-1.elb.amazonaws.com"
    ]

Our task definition looks like this:

    {
	"containerDefinitions": [
	    {
		"volumesFrom": [],
		"memory": 512,
		"portMappings": [
		    {
			"hostPort": 80,
			"containerPort": 80,
			"protocol": "tcp"
		    }
		],
		"essential": true,
		"entryPoint": [],
		"mountPoints": [],
		"name": "searchapp-webapp",
		"environment": [
		    {
			"name": "MEMCACHED_URL",
			"value": "internal-memcached-elb-47598939.us-east-1.elb.amazonaws.com"
		    },
		    {
			"name": "DB_PORT_80_TCP_ADDR",
			"value": "internal-mysql-elb-1700927984.us-east-1.elb.amazonaws.com"
		    },
		    {
			"name": "DB_ENV_MYSQL_PASSWORD",
			"value": "mysupersecretpassword"
		    },
		    {
			"name": "PASSENGER_APP_ENV",
			"value": "production"
		    },
		    {
			"name": "ELASTICSEARCH_URL",
			"value": "http://internal-elasticsearch-elb-907663077.us-east-1.elb.amazonaws.com"
		    },
		    {
			"name": "DB_ENV_MYSQL_NAME",
			"value": "searchapp_production"
		    },
		    {
			"name": "SECRET_KEY_BASE",
			"value": "6ada559ef69d8aa0501524f9ca8378122a5f25945d073e1c5aef0b88156fb9609b02fab9710b94343fef5f720f76a9619c54cbe93483749949fccce1b94e5fe0"
		    },
		    {
			"name": "DB_ENV_MYSQL_USER",
			"value": "root"
		    },
		    {
			"name": "REDIS_URL",
			"value": "internal-redis-elb-124984907.us-east-1.elb.amazonaws.com"
		    }
		],
		"links": [],
		"image": "pacuna/searchapp:1.1",
		"command": [],
		"cpu": 512,
	    }
	],
	"volumes": [],
	"family": "searchapp"
    }


As usual we also need an ELB for accessing our application. Let's create one now:

    $ aws elb create-load-balancer --load-balancer-name searchapp-elb --listeners Protocol=HTTP,LoadBalancerPort=80,InstanceProtocol=HTTP,InstancePort=80 --subnets subnet-39706b4e --security-groups sg-c70ff2a1

Output:

    {
	"DNSName": "searchapp-elb-1102698401.us-east-1.elb.amazonaws.com"
    }

Now, I have configured an special endpoint for the healthchecker of this ELB. This is because
once our application is up, we first need to index all the elasticsearch data. But the ELB
will try to ping the default healthcheck route which is '/' and that is going to return error, 
causing the task to be restarted. So instead we'll use the '/healthcheck' route which always
responds 200 as long as our webapp is up.

For configuring the health check endpoint let's run:


    $ aws elb configure-health-check --load-balancer-name searchapp-elb --health-check Target=HTTP:80/healthcheck,Interval=30,UnhealthyThreshold=2,HealthyThreshold=2,Timeout=3

Output:

    {
	"HealthCheck": {
	    "HealthyThreshold": 2,
	    "Interval": 30,
	    "Target": "HTTP:80/healthcheck",
	    "Timeout": 3,
	    "UnhealthyThreshold": 2
	}
    }

Another difference with this ELB is we don't want this to be an internal ELB, since is has to public
for people can access our application, so we don't need the internal flag.

We also have to expose the port 80 in our instances and in the ELB. Since all these resources are sharing
the same security group, the easiest way is to add rule for open access to port 80 on that security group (remember
to change the security group with yours):

    $ aws ec2 authorize-security-group-ingress --group-id sg-c70ff2a1 --protocol tcp --port 80 --cidr 0.0.0.0/0

The service json schema for our webapp looks like this:


    {
	"serviceName": "searchapp-service",
	"taskDefinition": "searchapp:1",
	"loadBalancers": [
	    {
		"loadBalancerName": "searchapp-elb",
		"containerName": "searchapp-webapp",
		"containerPort": 80
	    }
	],
	"desiredCount": 1,
	"role": "ecsServiceRole"
    }


Let's register our task definition:

    $ aws ecs register-task-definition --cli-input-json file://searchapp.json

Output:

    {
	"taskDefinition": {
	    "status": "ACTIVE",
	    "family": "searchapp",
	    "volumes": [],
	    "taskDefinitionArn": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp:1",
	    "containerDefinitions": [
		{
		    "environment": [
			{
			    "name": "MEMCACHED_URL",
			    "value": "internal-memcached-elb-47598939.us-east-1.elb.amazonaws.com"
			},
			{
			    "name": "DB_PORT_80_TCP_ADDR",
			    "value": "internal-mysql-elb-1700927984.us-east-1.elb.amazonaws.com"
			},
			{
			    "name": "DB_ENV_MYSQL_PASSWORD",
			    "value": "mysupersecretpassword"
			},
			{
			    "name": "PASSENGER_APP_ENV",
			    "value": "production"
			},
			{
			    "name": "ELASTICSEARCH_URL",
			    "value": "http://internal-elasticsearch-elb-907663077.us-east-1.elb.amazonaws.com"
			},
			{
			    "name": "DB_ENV_MYSQL_NAME",
			    "value": "searchapp_production"
			},
			{
			    "name": "SECRET_KEY_BASE",
			    "value": "6ada559ef69d8aa0501524f9ca8378122a5f25945d073e1c5aef0b88156fb9609b02fab9710b94343fef5f720f76a9619c54cbe93483749949fccce1b94e5fe0"
			},
			{
			    "name": "DB_ENV_MYSQL_USER",
			    "value": "root"
			},
			{
			    "name": "REDIS_URL",
			    "value": "internal-redis-elb-124984907.us-east-1.elb.amazonaws.com"
			}
		    ],
		    "name": "searchapp-webapp",
		    "links": [],
		    "mountPoints": [],
		    "image": "pacuna/searchapp:1.1",
		    "essential": true,
		    "portMappings": [
			{
			    "protocol": "tcp",
			    "containerPort": 80,
			    "hostPort": 80
			}
		    ],
		    "entryPoint": [],
		    "memory": 512,
		    "command": [],
		    "cpu": 512,
		    "volumesFrom": []
		}
	    ],
	    "revision": 1
	}
    }

And create the service:

    $ aws ecs create-service --cluster searchapp --service-name searchapp-service --cli-input-json file://searchapp-service.json

Output:

    {
	"service": {
	    "status": "ACTIVE",
	    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp:1",
	    "pendingCount": 0,
	    "loadBalancers": [
		{
		    "containerName": "searchapp-webapp",
		    "containerPort": 80,
		    "loadBalancerName": "searchapp-elb"
		}
	    ],
	    "roleArn": "arn:aws:iam::387705308362:role/ecsServiceRole",
	    "desiredCount": 1,
	    "serviceName": "searchapp-service",
	    "clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp",
	    "serviceArn": "arn:aws:ecs:us-east-1:387705308362:service/searchapp-service",
	    "deployments": [
		{
		    "status": "PRIMARY",
		    "pendingCount": 0,
		    "createdAt": 1444502776.876,
		    "desiredCount": 1,
		    "taskDefinition": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp:1",
		    "updatedAt": 1444502776.876,
		    "id": "ecs-svc/9223370592351998931",
		    "runningCount": 0
		}
	    ],
	    "events": [],
	    "runningCount": 0
	}
    }

Wait for few minutes, and if everything went ok, visit the dns of the searchapp-elb and you'll see our
empty application. In my case the endpoint is searchapp-elb-1102698401.us-east-1.elb.amazonaws.com;

![Empty Web Application](images/empty-app.png)

Great! if you can see the webapp, it means that the database connection and the migrations were
successful. We don't have data, since all the data for this sample application gets created through
database seeds. So we have to seed the database and the index the data.

We need to access to our application in order to seed the data.

A quick way to getting the DNS of the instance that's running this tasks is by describing the associated
ELB and query directly for the instance id:

    $ aws elb describe-load-balancers --load-balancer-name searchapp-elb --query "LoadBalancerDescriptions[*].Instances[*].InstanceId"

Output:

    [
	[
	    "i-8372af57"
	]
    ]


And then we can describe the instance using the instance id and query for the dns:

    $ aws ec2 describe-instances --instance-ids i-8372af57 --query "Reservations[*].Instances[*].PublicDnsName"

Output:

    [
	[
	    "ec2-52-23-234-174.compute-1.amazonaws.com"
	]
    ]

Cool, let's connect to that instance:

    $ ssh -i ~/.ssh/keys/ClusterKeyPair.pem ec2-user@ec2-52-23-234-174.compute-1.amazonaws.com

       __|  __|  __|
       _|  (   \__ \   Amazon ECS-Optimized Amazon Linux AMI 2015.03.g
     ____|\___|____/

    For documentation visit, http://aws.amazon.com/documentation/ecs
    No packages needed for security; 1 packages available
    Run "sudo yum update" to apply all updates.
    Amazon Linux version 2015.09 is available.
    -bash: warning: setlocale: LC_CTYPE: cannot change locale (UTF-8): No such file or directory
    [ec2-user@ip-172-31-1-131 ~]$

If you run docker ps in your instance, you'll see that the webapp is running:

    $ docker ps
    CONTAINER ID        IMAGE                            COMMAND                CREATED             STATUS              PORTS                         NAMES
    f3831182e38d        pacuna/searchapp:1.1             "/sbin/my_init"        22 minutes ago      Up 22 minutes       0.0.0.0:80->80/tcp, 443/tcp   ecs-searchapp-1-searchapp-webapp-cccfafcefcb7f589a201
    efd0080d60b9        memcached                        "/entrypoint.sh memc   4 hours ago         Up 4 hours          0.0.0.0:11211->11211/tcp      ecs-memcached-1-searchapp-memcached-fc96dee99bf0828c2c00
    957d272d3081        amazon/amazon-ecs-agent:latest   "/agent"               16 hours ago        Up 16 hours         127.0.0.1:51678->51678/tcp    ecs-agent

Now can execute commands in that container as usual. My preferred way is to go inside the container and run 
the commands there. We can do this by running (replace the container id with the one in your output):

    $ docker exec -it f3831182e38d bash

    root@f3831182e38d:/home/app/webapp#

Now we are inside of the rails application. You run all the typical commands, rails console, rake tasks, tail
the log, etc. Just keep in mind that you're in production environment.

Now, before running the seeds, I've had some troubles with the database collation, but they are really easy to
fix, we just have to recreate our database, connect to it and run a MySQL command.

First let's create our database:

    $ RAILS_ENV=production rake db:drop
    $ RAILS_ENV=production rake db:create
    $ RAILS_ENV=production rake db:migrate

In this container we have MySQL available, so we can connect right from here. Look for your MySQL load balancer
DNS address (look in your searchapp task definition):

    $ mysql -h internal-mysql-elb-1700927984.us-east-1.elb.amazonaws.com -u root -pmysupersecretpassword -P 80
    Welcome to the MySQL monitor.  Commands end with ; or \g.
    Your MySQL connection id is 1119
    Server version: 5.7.8-rc MySQL Community Server (GPL)

    Copyright (c) 2000, 2015, Oracle and/or its affiliates. All rights reserved.

    Oracle is a registered trademark of Oracle Corporation and/or its
    affiliates. Other names may be trademarks of their respective
    owners.

    Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

    mysql>

And in the MySQL console run:

    ALTER DATABASE searchapp_production CHARACTER SET utf8 COLLATE utf8_general_ci;


Output:

    Query OK, 1 row affected (0.00 sec)


And that's it. Run exit to get back to the container, and now run the seeds:

    $ RAILS_ENV=production rake db:seed

You'll see a lot of output for the database being inserted into the database. Just wait
until finishes.

Once it's ready, we can index the data using the task provided by the elasticsearch-rails gem
(you can use this task anytime you change your index or you want to reindex your data):

    $ RAILS_ENV=production bundle exec rake environment elasticsearch:import:model CLASS='Article' FORCE='y' BATCH=100

Output:

    [IMPORT] Done

Now if you visit the web page of the application again, you'll see that we have all the data:

![Complete Web Application](images/complete-app.png)

That's great. So right now we're pretty sure that MySQL and Elasticsearch are working correctly.
But what about redis and memcached? Well, the project comes with a worker ready for use.
The worker will index the data using sidekiq every time a record gets modified.

First let's run the worker:

    root@f3831182e38d:/home/app/webapp# RAILS_ENV=production bundle exec sidekiq --queue elasticsearch --verbose

The console will be attached while we have sidekiq running, we just want to check if the hook with redis is correct:

If you see the first line of the output in the console, you should see something like:

    2015-10-10T19:36:16.169Z 1377 TID-a8c4g INFO: Booting Sidekiq 3.3.4 with redis options {:url=>"redis://internal-redis-elb-124984907.us-east-1.elb.amazonaws.com:80/12"}

Which means that the conection was successful.

For memcached, let's hit Ctrl-c for exiting sidekiq and open a rails console:

    root@f3831182e38d:/home/app/webapp# rails c production

And let's clean the cache:

    irb(main):001:0> Rails.cache.clear
    Cache clear: flushing all keys
    Dalli::Server#connect internal-memcached-elb-47598939.us-east-1.elb.amazonaws.com:80
    => [true]

You can see that the connection with memcached is working properly.


# Deployment

## Introduction

## Setting up the cluster

### Creating the cluster

We can create the cluster using one simple command. The only parameter
that we need is the cluster name.

{linenos=off}
	$ aws ecs create-cluster --cluster-name "searchapp"

Output:

{linenos=off}
	{
	    "cluster": {
		"status": "ACTIVE",
		"clusterName": "searchapp",
		"registeredContainerInstancesCount": 0,
		"pendingTasksCount": 0,
		"runningTasksCount": 0,
		"activeServicesCount": 0,
		"clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp"
	    }
	}

The cluster was created successfully, it has zero EC2 container
instances registered, zero task running and no services.

The idea behind the ECS cluster is that you can forget about where and how you're
task and services will be deployed. You just have to launch them and Amazon
will take care of assigning the service or task to an instance. The only thing
that we have to take care for now is adding some EC2 instances to the cluster.

### Launching the EC2 instances

Now that we have our cluster, we can launch some instances and register them
in our cluster. For launching the instances we'll need:

- A Key Pair for ssh access
- A Security Group

Let's create our key pair:

{linenos=off}
	$ aws ec2 create-key-pair --key-name keys_cluster

Output:

{linenos=off}
	{
	    "KeyMaterial": "-----BEGIN RSA PRIVATE KEY-----\nTHEPRIVATEKEY==\n-----END RSA PRIVATE KEY-----",
	    "KeyName": "keys_cluster",
	    "KeyFingerprint": "69:90:35:93:a8:db:fe:d0:77:00:82:35:b6:84:2a:98:6d:43:30:b5"
	}

Create a file named key_cluster.pem and paste the entire key (the value of the KeyMaterial key).
Don't forget the lines:

{linenos=off}
	"----BEGIN RSA PRIVATE KEY----"
	"-----END RSA PRIVATE KEY-----" 

Now set the permissions of the file:

{linenos=off}
	$ chmod 400 cluster.pem

Now move the file to your ssh keys directory or some place safe. Remember
that with just this file you'll be able to ssh into your cluster instances.

For the security groups for the instances, we can use our default security group
for our default vpc. In order to get the information for our default vpc security
group, you can run:

{linenos=off}
	$ aws ec2 describe-security-groups

Output:

{linenos=off}
	{
	    "SecurityGroups": [
		{
		    "IpPermissionsEgress": [
			{
			    "IpProtocol": "-1",
			    "IpRanges": [
				{
				    "CidrIp": "0.0.0.0/0"
				}
			    ],
			    "UserIdGroupPairs": [],
			    "PrefixListIds": []
			}
		    ],
		    "Description": "default VPC security group",
		    "IpPermissions": [
			{
			    "IpProtocol": "-1",
			    "IpRanges": [],
			    "UserIdGroupPairs": [
				{
				    "UserId": "387705308362",
				    "GroupId": "sg-c70ff2a1"
				}
			    ],
			    "PrefixListIds": []
			}
		    ],
		    "GroupName": "default",
		    "VpcId": "vpc-120cd476",
		    "OwnerId": "387705308362",
		    "GroupId": "sg-c70ff2a1"
		}
	    ]
	}

As you can see from the output, my default vpc group has the id sg-c70ff2a1 and belongs
to the vpc with id vpc-120cd476. We will se some of this data during the next steps.
This security group allows all traffic between resources that belongs to the group.
The only thing that we are going to need is access to the port 22 for getting
ssh access to our instances. We'll do that later.


Our instances are going to need an IAM Role in order to allow certain actions
on the ECS infrastructure. Lucky for us, amazon has already defined the set
of policies that we need. So we just have to create the Role, attach the 
policy, and then add that role to an instance profile so we can add the
Role on launching time of our instances.

In the deploy folder of the project, you'll find an standard trust policy for
creating our role. Just copy the file and run this command replacing your
path to the file.

{linenos=off}
	$ aws iam create-role --role-name ecsInstanceRole --assume-role-policy-document file://policies/AmazonEC2ContainerServiceforEC2Role-Trust-Policy.json

Output:

{linenos=off}
	{
	    "Role": {
		"AssumeRolePolicyDocument": {
		    "Version": "2012-10-17",
		    "Statement": {
			"Action": "sts:AssumeRole",
			"Effect": "Allow",
			"Principal": {
			    "Service": "ec2.amazonaws.com"
			}
		    }
		},
		"RoleId": "AROAIIH2CKLXDXCQ3YS4Q",
		"CreateDate": "2015-10-08T23:01:52.285Z",
		"RoleName": "ecsInstanceRole",
		"Path": "/",
		"Arn": "arn:aws:iam::387705308362:role/ecsInstanceRole"
	    }
	}

Now, let's attach the policy to our new ecsInstanceRole role:

{linenos=off}
	$ aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role --role-name ecsInstanceRole

The final step is to create the instace profile and attach the Role to that profile:

{linenos=off}
	$ aws iam create-instance-profile --instance-profile-name ecsInstanceRole

Output:

{linenos=off}
	{
	    "InstanceProfile": {
		"InstanceProfileId": "AIPAI5CUZHG54Y7P6V5SG",
		"Roles": [],
		"CreateDate": "2015-10-08T23:04:44.535Z",
		"InstanceProfileName": "ecsInstanceRole",
		"Path": "/",
		"Arn": "arn:aws:iam::387705308362:instance-profile/ecsInstanceRole"
	    }
	}

And attaching...

{linenos=off}
	$ aws iam add-role-to-instance-profile --instance-profile-name ecsInstanceRole --role-name ecsInstanceRole


This command doesn't return any output.

Now we are ready to launch the instances for the cluster.

The AMI that we'll use is amzn-ami-2015.03.g-amazon-ecs-optimized with the id
ami-4fe4852a. This image comes ready and optimized for ECS.

We are going to launch 2 instances of type t2.micro and we are going to use
the keys and instance Role that were created previously, one of our subnets and
the security group for the default vpc.

For getting the available subnets, run:

{linenos=off}
	$ aws ec2 describe-subnets

Output:

{linenos=off}
	{
	    "Subnets": [
		{
		    "VpcId": "vpc-120cd476",
		    "CidrBlock": "172.31.0.0/20",
		    "MapPublicIpOnLaunch": true,
		    "DefaultForAz": true,
		    "State": "available",
		    "AvailabilityZone": "us-east-1d",
		    "SubnetId": "subnet-39706b4e",
		    "AvailableIpAddressCount": 4091
		},
		{
		    "VpcId": "vpc-120cd476",
		    "CidrBlock": "172.31.16.0/20",
		    "MapPublicIpOnLaunch": true,
		    "DefaultForAz": true,
		    "State": "available",
		    "AvailabilityZone": "us-east-1a",
		    "SubnetId": "subnet-fa724fa3",
		    "AvailableIpAddressCount": 4091
		},
		{
		    "VpcId": "vpc-120cd476",
		    "CidrBlock": "172.31.48.0/20",
		    "MapPublicIpOnLaunch": true,
		    "DefaultForAz": true,
		    "State": "available",
		    "AvailabilityZone": "us-east-1c",
		    "SubnetId": "subnet-4660426d",
		    "AvailableIpAddressCount": 4091
		},
		{
		    "VpcId": "vpc-120cd476",
		    "CidrBlock": "172.31.32.0/20",
		    "MapPublicIpOnLaunch": true,
		    "DefaultForAz": true,
		    "State": "available",
		    "AvailabilityZone": "us-east-1e",
		    "SubnetId": "subnet-477ea97a",
		    "AvailableIpAddressCount": 4091
		}
	    ]
	}

I have 4 available subnets, all in my default vpc. I'll choose the first
one for launching the instances.

In the following command replace the security-groups-ids with your default
vpc security group id, your subnet-id and the Arn of the instance profile which
you can find in the output of the create-instance-profile.

The user-data parameter allow us to add some initial configuration to the instances.
In our case we are passing the necessary configuration for registering the instances
in our searchapp cluster. The user-data.sh file looks like this (also in the deploy folder):

{linenos=off}
	#!/bin/bash
	echo ECS_CLUSTER=searchapp >> /etc/ecs/ecs.config

Now we can run the command for launching the instances:

{linenos=off}
	$ aws ec2 run-instances --image-id ami-4fe4852a --count 2 --instance-type t2.micro --key-name keys_cluster --security-group-ids sg-c70ff2a1 --subnet-id subnet-39706b4e  --iam-instance-profile Arn=arn:aws:iam::387705308362:instance-profile/ecsInstanceRole --associate-public-ip-address --user-data file://user-data/user-data.sh

Output:

{linenos=off}
	{
	    "OwnerId": "387705308362",
	    "ReservationId": "r-9dcb8660",
	    "Groups": [],
	    "Instances": [
		{
		    "Monitoring": {
			"State": "disabled"
		    },
		    "PublicDnsName": "",
		    "RootDeviceType": "ebs",
		    "State": {
			"Code": 0,
			"Name": "pending"
		    },
		    "EbsOptimized": false,
		    "LaunchTime": "2015-10-08T23:21:46.000Z",
		    "PrivateIpAddress": "172.31.5.238",
		    "ProductCodes": [],
		    "VpcId": "vpc-120cd476",
		    "StateTransitionReason": "",
		    "InstanceId": "i-f7b07a23",
		    "ImageId": "ami-4fe4852a",
		    "PrivateDnsName": "ip-172-31-5-238.ec2.internal",
		    "KeyName": "keys_cluster",
		    "SecurityGroups": [
			{
			    "GroupName": "default",
			    "GroupId": "sg-c70ff2a1"
			}
		    ],
		....
		....

You can poll the status of the launched instances using the command

{linenos=off}
	$ aws ec2 describe-instances


After a few minutes, try running this command for polling the state of the cluster
and see if the instances were correctly registered:

{linenos=off}
	$ aws ecs describe-clusters --clusters searchapp

Output:

{linenos=off}
	{
	    "clusters": [
		{
		    "status": "ACTIVE",
		    "clusterName": "searchapp",
		    "registeredContainerInstancesCount": 2,
		    "pendingTasksCount": 0,
		    "runningTasksCount": 0,
		    "activeServicesCount": 0,
		    "clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp"
		}
	    ],
	    "failures": []
	}

Great, the output is telling me that the searchapp cluster has 2 registered instances.

## Tasks definitions for the dependencies

### MySQL

The task definition will contain the information of the container that you
will run. You can create the task definition using json format, and the register 
the task in your cluster using the aws cli.
You can find all the task defitions used in this book in the deploy/task-definitions
directory of the source code repository.

{linenos=off}
{
    "containerDefinitions": [
        {
            "volumesFrom": [],
            "memory": 256,
            "portMappings": [
                {
                    "hostPort": 3306,
                    "containerPort": 3306,
                    "protocol": "tcp"
                }
            ],
            "essential": true,
            "entryPoint": [],
            "mountPoints": [
                {
                    "containerPath": "/var/lib/mysql",
                    "sourceVolume": "mysql-data",
                    "readOnly": false
                }
            ],
            "name": "searchapp-db",
            "environment": [
                {
                    "name": "MYSQL_DATABASE",
                    "value": "searchapp_production"
                },
                {
                    "name": "MYSQL_PASSWORD",
                    "value": "mysecretpassword"
                },
                {
                    "name": "MYSQL_ROOT_PASSWORD",
                    "value": "mysupersecretpassword"
                },
                {
                    "name": "MYSQL_USER",
                    "value": "searchappusr"
                }
            ],
            "links": [],
            "image": "mysql:5.7",
            "command": [],
            "cpu": 128
        }
    ],
    "volumes": [
        {
            "name": "mysql-data"
        }
    ],
    "family": "searchapp-db"
}

This template might look complicated, but there's a lot of irrelevant information
there. Actually, if you use the ECS console for registering task definitions, you'll
see that most of the information can be ommited. 
If you have used docker-compose in the past, you'll see some similaties between
the structure of the definition. Here you also have port mapping, environment
variables, links with other containers, and volumes for persisting container data.
This task is mapping the port 3306 to the host, declaring the environment variables
that the mysql image requires, and creating a volume for mounting the /var/lib/mysql
directory. This approach will allow us to avoid data lost when the mysql container
is restarted. The problem is that we are now coupled to an particular instance, since
ECS doesn't sync the data between instances. This is still a dark area in the containers
world and it's why some people prefer to run only stateless application on containers
and use more typical approaches for their database resources.

The command for register this task in our cluster is:

{linenos=off}
	$ aws ecs register-task-definition --cli-input-json file://task-definitions/mysql.json

Output:

{linenos=off}
	{
	    "taskDefinition": {
		"status": "ACTIVE",
		"family": "searchapp-db",
		"volumes": [
		    {
			"host": {},
			"name": "mysql-data"
		    }
		],
		"taskDefinitionArn": "arn:aws:ecs:us-east-1:387705308362:task-definition/searchapp-db:1",
		"containerDefinitions": [
		    {
			"environment": [
			    {
				"name": "MYSQL_DATABASE",
				"value": "searchapp_production"
			    },
			    {
				"name": "MYSQL_PASSWORD",
				"value": "mysecretpassword"
			    },
			    {
				"name": "MYSQL_ROOT_PASSWORD",
				"value": "mysupersecretpassword"
			    },
			    {
				"name": "MYSQL_USER",
				"value": "searchappusr"
			    }
			],
			"name": "searchapp-db",
			"links": [],
			"mountPoints": [
			    {
				"sourceVolume": "mysql-data",
				"readOnly": false,
				"containerPath": "/var/lib/mysql"
			    }
			],
			"image": "mysql:5.7",
			"essential": true,
			"portMappings": [
			    {
				"protocol": "tcp",
				"containerPort": 3306,
				"hostPort": 3306
			    }
			],
			"entryPoint": [],
			"memory": 256,
			"command": [],
			"cpu": 128,
			"volumesFrom": []
		    }
		],
		"revision": 1
	    }
	}

Great, our task was created successfully. Now every time we make changes
to this taks, a new revision will be created as long as we use the same
family value when registering the new json file.
Also, now we can run a service scheduler for running a task using this task definition.
The service will be in charge of allocating the task somewhere in the cluster and
restarting the task if the container fail for some reason.
Since the service associated with this task will deploy the container anywhere
in the cluster, we need some mechanism for getting some static value that will
point to the task, in this case the mysql server. The native way to accomplish
this in ECS is to create an Elastic Load Balancer and then when you define the service
you can add an option for attaching it to the ELB. This way, even is the service
allocates the task in a different instance between deploys, it will always be
attached to the ELB.

Let's create an ELB for the MySQL container. Remember to change the subnet and the
security group with your default or custom vpc information.

{linenos=off}
	$ aws elb create-load-balancer --load-balancer-name mysql-elb --listeners "Protocol=TCP,LoadBalancerPort=80,InstanceProtocol=TCP,InstancePort=3306" --subnets subnet-39706b4e --security-groups sg-c70ff2a1

Output:

{linenos=off}
	{
	    "DNSName": "mysql-elb-1267740639.us-east-1.elb.amazonaws.com"
	}

The output is telling us where we should point our database url configuration in our Rails application. This
ELB will be pointing to an EC2 instance, in which the container will be running and mapping
its 3306 to the 3306 port of the instance.

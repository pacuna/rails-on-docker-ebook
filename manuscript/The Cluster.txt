# The Cluster

## Creating the cluster

Let's start by creating the central piece in the ECS architecture, the cluster.
We can create the cluster using one simple command. The only parameter
that we need is the name:

	$ aws ecs create-cluster --cluster-name "searchapp"

Output:

	{
	    "cluster": {
		"status": "ACTIVE",
		"clusterName": "searchapp",
		"registeredContainerInstancesCount": 0,
		"pendingTasksCount": 0,
		"runningTasksCount": 0,
		"activeServicesCount": 0,
		"clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp"
	    }
	}

The cluster was created successfully, and as you can see in the output 
it has zero EC2 container instances registered, zero task running and no services.

The idea behind the ECS cluster is that you can forget about where and how you're
tasks and services will be deployed. You just have to launch them and Amazon
will take care of assigning the service or task to an instance. The only thing
that we have to do now is to add some EC2 instances to the cluster.

## Launching the EC2 instances

Now that we have our cluster, we can launch a couple of instances and register them
in the cluster. For launching the instances we'll need:

- A Key Pair for ssh access
- A Security Group
- An Instance Role

Let's start by creating the key pair. We can create the key and send the output to
a .pem file using this command:

	$ aws ec2 create-key-pair --key-name ClusterKeyPair --query 'KeyMaterial' --output text > ClusterKeyPair.pem

Set the permissions of the file:

	$ chmod 400 ClusterKeyPair.pem

And move the file to your ssh keys directory or some place safe. Remember that
this file will be necessary for accessing the instances via ssh.

For the instances security group, we can use the security group that comes
for the default vpc. In order to get the information for your default vpc security
group, run:

	$ aws ec2 describe-security-groups

Output:

	{
	    "SecurityGroups": [
		{
		    "IpPermissionsEgress": [
			{
			    "IpProtocol": "-1",
			    "IpRanges": [
				{
				    "CidrIp": "0.0.0.0/0"
				}
			    ],
			    "UserIdGroupPairs": [],
			    "PrefixListIds": []
			}
		    ],
		    "Description": "default VPC security group",
		    "IpPermissions": [
			{
			    "IpProtocol": "-1",
			    "IpRanges": [],
			    "UserIdGroupPairs": [
				{
				    "UserId": "387705308362",
				    "GroupId": "sg-c70ff2a1"
				}
			    ],
			    "PrefixListIds": []
			}
		    ],
		    "GroupName": "default",
		    "VpcId": "vpc-120cd476",
		    "OwnerId": "387705308362",
		    "GroupId": "sg-c70ff2a1"
		}
	    ]
	}

As you can see from the output, my default vpc group has the id sg-c70ff2a1 and belongs
to the vpc with id vpc-120cd476. We will use some of this information through the next steps.
This security group allows all traffic between resources that belongs to the group.
That's good but we also going to need access to the port 22 for getting
ssh access to our instances. We'll do that later.

Our instances are also going to need an IAM Role in order to allow certain actions
on the ECS infrastructure. Lucky for us, Amazon AWS has already defined the set
of policies that we need in a native policy. We only have to create the Role, attach this
policy, and then add that Role to an instance profile so we can add the
Role while the instances are being launched.

In the deploy folder of the project, you'll find an standard trust policy for
creating the Role. The name of the file is AmazonEC2ContainerServiceforEC2Role-Trust-Policy.json.
Run this command replacing the path to the file:

	$ aws iam create-role --role-name ecsInstanceRole --assume-role-policy-document file://policies/AmazonEC2ContainerServiceforEC2Role-Trust-Policy.json

Output:

	{
	    "Role": {
		"AssumeRolePolicyDocument": {
		    "Version": "2012-10-17",
		    "Statement": {
			"Action": "sts:AssumeRole",
			"Effect": "Allow",
			"Principal": {
			    "Service": "ec2.amazonaws.com"
			}
		    }
		},
		"RoleId": "AROAIIH2CKLXDXCQ3YS4Q",
		"CreateDate": "2015-10-08T23:01:52.285Z",
		"RoleName": "ecsInstanceRole",
		"Path": "/",
		"Arn": "arn:aws:iam::387705308362:role/ecsInstanceRole"
	    }
	}

Now we attach the policy to our new ecsInstanceRole role:

	$ aws iam attach-role-policy --policy-arn arn:aws:iam::aws:policy/service-role/AmazonEC2ContainerServiceforEC2Role --role-name ecsInstanceRole

create the instance profile:

	$ aws iam create-instance-profile --instance-profile-name ecsInstanceRole

Output:

	{
	    "InstanceProfile": {
		"InstanceProfileId": "AIPAI5CUZHG54Y7P6V5SG",
		"Roles": [],
		"CreateDate": "2015-10-08T23:04:44.535Z",
		"InstanceProfileName": "ecsInstanceRole",
		"Path": "/",
		"Arn": "arn:aws:iam::387705308362:instance-profile/ecsInstanceRole"
	    }
	}

and finally attach the Role to that profile:

	$ aws iam add-role-to-instance-profile --instance-profile-name ecsInstanceRole --role-name ecsInstanceRole

Now we are ready to launch the instances for the cluster.

The AMI that we'll use it's called amzn-ami-2015.03.g-amazon-ecs-optimized with id
ami-4fe4852a. This image comes ready and optimized for ECS.

We are launching 2 instances of type t2.micro and we are going to use
the keys and instance Role that were created previously, one of our subnets and
the security group for the default vpc.

For getting the available subnets, run:

	$ aws ec2 describe-subnets

Output:

	{
	    "Subnets": [
		{
		    "VpcId": "vpc-120cd476",
		    "CidrBlock": "172.31.0.0/20",
		    "MapPublicIpOnLaunch": true,
		    "DefaultForAz": true,
		    "State": "available",
		    "AvailabilityZone": "us-east-1d",
		    "SubnetId": "subnet-39706b4e",
		    "AvailableIpAddressCount": 4091
		},
		{
		    "VpcId": "vpc-120cd476",
		    "CidrBlock": "172.31.16.0/20",
		    "MapPublicIpOnLaunch": true,
		    "DefaultForAz": true,
		    "State": "available",
		    "AvailabilityZone": "us-east-1a",
		    "SubnetId": "subnet-fa724fa3",
		    "AvailableIpAddressCount": 4091
		},
		{
		    "VpcId": "vpc-120cd476",
		    "CidrBlock": "172.31.48.0/20",
		    "MapPublicIpOnLaunch": true,
		    "DefaultForAz": true,
		    "State": "available",
		    "AvailabilityZone": "us-east-1c",
		    "SubnetId": "subnet-4660426d",
		    "AvailableIpAddressCount": 4091
		},
		{
		    "VpcId": "vpc-120cd476",
		    "CidrBlock": "172.31.32.0/20",
		    "MapPublicIpOnLaunch": true,
		    "DefaultForAz": true,
		    "State": "available",
		    "AvailabilityZone": "us-east-1e",
		    "SubnetId": "subnet-477ea97a",
		    "AvailableIpAddressCount": 4091
		}
	    ]
	}

I have 4 available subnets. We can do very advanced stuff using different subnets
depending on the resources that we want to launch, but in our case let's use just
one for all the resources.  I'll choose the first one for launching the instances.

In the following command replace the security-groups-ids with your default
vpc security group id, the subnet-id with your subnet-id and the Arn of the instance profile which
you can find in the output of the create-instance-profile.

The user-data parameter allow us to add some initial configuration to the instances.
In our case we are passing the necessary configuration for registering the instances
in our searchapp cluster. The user-data.sh file contains just these two lines:

	#!/bin/bash
	echo ECS_CLUSTER=searchapp >> /etc/ecs/ecs.config

You can find this file in the deploy folder of the source code.
Now we can run the command for launching the instances:

	$ aws ec2 run-instances --image-id ami-4fe4852a --count 2 --instance-type t2.micro --key-name ClusterKeyPair --security-group-ids sg-c70ff2a1 --subnet-id subnet-39706b4e  --iam-instance-profile Arn=arn:aws:iam::387705308362:instance-profile/ecsInstanceRole --associate-public-ip-address --user-data file://user-data/user-data.sh

Output:

    {
	"OwnerId": "387705308362",
	"ReservationId": "r-202269dd",
	"Groups": [],
	"Instances": [
	    {
		"Monitoring": {
		    "State": "disabled"
		},
		"PublicDnsName": "",
		"RootDeviceType": "ebs",
		"State": {
		    "Code": 0,
		    "Name": "pending"
		},
		"EbsOptimized": false,
		"LaunchTime": "2015-10-10T02:13:50.000Z",
		"PrivateIpAddress": "172.31.1.132",
		"ProductCodes": [],
		"VpcId": "vpc-120cd476",
		"StateTransitionReason": "",
		"InstanceId": "i-8272af56",
		"ImageId": "ami-4fe4852a",
		"PrivateDnsName": "ip-172-31-1-132.ec2.internal",
		"KeyName": "ClusterKeyPair",
		"SecurityGroups": [
		    {
			"GroupName": "default",
			"GroupId": "sg-c70ff2a1"
		    }
		],
		"ClientToken": "",
		"SubnetId": "subnet-39706b4e",
		"InstanceType": "t2.micro",
		...

You can poll the status of the launched instances using the command

	$ aws ec2 describe-instances

After a few minutes, try running this command for polling the state of the cluster
and see if the instances were correctly registered:

	$ aws ecs describe-clusters --clusters searchapp

Output:

	{
	    "clusters": [
		{
		    "status": "ACTIVE",
		    "clusterName": "searchapp",
		    "registeredContainerInstancesCount": 2,
		    "pendingTasksCount": 0,
		    "runningTasksCount": 0,
		    "activeServicesCount": 0,
		    "clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp"
		}
	    ],
	    "failures": []
	}

Great, we have 2 registered instances in the searchapp cluster.

# Setting up the CI Environment

## Testing our application

Our application has already a nice set of tests. I fixed some minor issues
that I found running the tests, so you should be able to run them with
no problems.

It's important to always use `RAILS_ENV=test` when running the test suite.
That's because the phusion passenger image overrides this variable using the one
that you pass when you run the container. This way, in development, it's
always going to be development, even if you declare that you want
to use the test environment in your test helper. 

With that being said, you can run the test suite by using:

    RAILS_ENV=test rake

Output:

    root@8114b51c8ae3:/home/app/webapp# RAILS_ENV=test rake
    Run options: --seed 36129

    # Running:

    .......................................

    Finished in 4.600963s, 8.4765 runs/s, 19.9958 assertions/s.

    39 runs, 92 assertions, 0 failures, 0 errors, 0 skips

## Installing Docker

Our CI environment will be powered by Jenkins. We'll use Jenkins for building
our testing and production images, pushing the production image to DockerHub
and finally to ECS. We are going to use a couple of Jenkins plugins for some of these steps.

So, assuming that you have an installation of Jenkins (if not, you can
find a lot of tutorials out there for installing Jenkins on EC2). I prefer to run
Jenkins in a regular server instead of containers just because I think is not necessary
to add that complexity (run containers inside containers).

First we need to install docker in your machine. You can use the package manager
of your EC2 Linux distribution or download the binaries from the Docker website.
In my case I'm running the Amazon Linux ami, so I can use yum to install docker:

    # yum update -y
    # yum install -y docker

Let's add the jenkins user to the docker group:

    # usermod -a -G docker jenkins

Now we can start the docker service and make them available after a reboot:

    # service docker start
    # chkconfig docker on

## Installing the plugins

In the Jenkins plugin manager, search and install the following plugins:

    - CloudBees Docker Build and Publish plugin
    - CloudBees Docker Hub Notification
    - Github plugin

Remember to restart Jenkins after the installation.

With these 3 plugins we can pull the source code from our GitHub account, build
the Docker image and push it to DockerHub. Then, with some shell magic, we'll
update the ECS task definitions and the service. This way we are going to be able
to deploy to production with just a push to the master branch (or the one you choose).

## Installing the aws cli

We will be sending some requests to the Amazon ECS api in order to register
the updated task definitions and for update the services. So we are going to need
the aws cli tool on the Jenkins server and add the credentials to the aws configuration.

Once you installed the aws cli, run `aws configure` an add your API credentials.
You can obtain these using the web console in your AWS account.

    # sudo -su jenkins
    > aws configure

## Login to DockerHub

Jenkins also needs permission to push the images to DockerHub, so you have to
login with your DockerHub credentials using docker login.

    # docker login

## Creating the project

Go to "create new Job" and select Freestyle project. Choose a name for your project.

![Jenkins Freestyle project](images/freestyle-project.png)

Next, in the project configuration, add the url to the GitHub repository
that contains the project:

![GitHub Repo configuration](images/jenkins-config-1.png)

Under the Source Code Management Section, select git and add your credentials.
Use the branch that you like to deploy:

![GitHub Repo configuration 2](images/jenkins-config-2.png)

In the Build Triggers section, select "Build when a change is pushed to GitHub"

![Jenkins configuration](images/jenkins-config-3.png)

In the Build section, select Docker Build And Publish and use these parameters:

![Docker Build And Publish](images/jenkins-config-4.png)

Remember to change the username of the DockerHub repository.

This docker build doesn't need to be pushed since it's going to be just for running the
test suite. If the tests pass, then we build the image that's going to
be pushed to DockerHub and finally to ECS.

## Running the tests

Add another build step, but this time select "Execute Shell" and add the following
command:

![Shell command](images/jenkins-config-5.png)

This will execute a file that's in the deploy folder. Let's checkout
the script:

    #!/bin/bash
    docker pull mysql:5.7
    docker pull elasticsearch
    docker pull redis
    docker run --name mysql -e MYSQL_ROOT_PASSWORD=secretpassword -d mysql:5.7
    docker run --name elasticsearch -d elasticsearch
    docker run --name redis -d redis

    sleep 15s
    docker run --name searchapp-test -e PASSENGER_APP_ENV=test --link mysql:db --link elasticsearch:es --link redis:redis --entrypoint="./deploy/run_tests.sh" -t pacuna/searchapp:test_v_${BUILD_NUMBER} | perl -pe '/Tests failed./ && `echo -n "Tests failed" > tests-failed-flag`'
    if [ ! -f tests-failed-flag ]; then
      echo -e "Tests passed."
    else
      echo -e "Build failed since tests failed."
      rm tests-failed-flag
      echo 'removing test containers'
      docker rm -f searchapp-test
      docker rm -f mysql
      docker rm -f elasticsearch
      docker rm -f redis
      exit 1
    fi
    echo 'removing test containers'
    docker rm -f searchapp-test
    docker rm -f mysql
    docker rm -f elasticsearch
    docker rm -f redis

This command sequence is actually pretty simple. First we pull the necessary
dependencies for the test suite, which are mysql, elasticsearch and redis.
Then we run containers for each one of these dependencies.
The sleep command is just for give the containers some time to start before
we can link them with our application. Then we run our application that
was previously pulled from the GitHub repository. We pass the phusion passenger
environment variable and create links with the other containers. We also
pass a custom entrypoint which is just a simple script for setting up the test database and 
running the tests. The script also outputs the exit code so we can know if the tests failed or not.

    #!/bin/bash
    echo 'creating test database...'
    RAILS_ENV=test rake db:create
    echo 'migrating test database...'
    RAILS_ENV=test rake db:migrate
    echo 'running tests...'
    RAILS_ENV=test rake

    return_code=$?
    if [[ $return_code != 0 ]] ; then
      echo -e "Tests failed."
      exit $return_code
    fi


Click apply and save.

Now if you build the project and inspect the console output you'll see that jenkins
is going to pull the code from GitHub, build the searchapp-test image and
run the script. 

Finally you should see something like this:

![Test results](images/test-results.png)

Great, that means that the shell build step will block or allow the CI pipeline to continue
depending on the test results. 
Now we have to create a new step for build the image that will be pushed
to DockerHub.

## Pushing to DockerHub

Same way as before, in the configuration of the project, add a new Docker build
and publish step but time don't skip the push. 

![Pushing to DockerHub](images/pushing-to-dockerhub.png)

Now, create another execute shell build step and the following line:

    ./deploy/update_service.sh

![Shell command](images/jenkins-config-6.png)

Finally we need a build step for cleaning up the old images. If you don't add
this step, you'll ended up with a lot of unused images on your jenkins server.
Add the following line in a shell execution step:

    ./deploy/clean_images.sh

![Clean images](images/jenkins-config-7.png)

And click apply and save.

## Adding another EC2 instance

Right now we have a cluster with 2 t2.micro EC2 instances. We are
going to need one more. So let's run our command for creating another
instance. Remember to use your security group and subnet ids:

    $ aws ec2 run-instances --image-id ami-4fe4852a --count 1 --instance-type t2.micro --key-name ClusterKeyPair --security-group-ids sg-c70ff2a1 --subnet-id subnet-39706b4e  --iam-instance-profile Arn=arn:aws:iam::387705308362:instance-profile/ecsInstanceRole --associate-public-ip-address --user-data file://user-data/user-data.sh

Now, once the instance is ready, you should see that you have 3 instances
registered in the cluster:

    $ aws ecs describe-clusters --cluster searchapp

Output:

    {
	"clusters": [
	    {
		"status": "ACTIVE",
		"clusterName": "searchapp",
		"registeredContainerInstancesCount": 3,
		"pendingTasksCount": 0,
		"runningTasksCount": 5,
		"activeServicesCount": 5,
		"clusterArn": "arn:aws:ecs:us-east-1:387705308362:cluster/searchapp"
	    }
	],
	"failures": []
    }

One thing to notice, is that once the third instance is available for the cluster,
since we have 2 instances being heavily used, the scheduler will probably move
some tasks between the instances. So maybe your MySQL service, or your elasticsearch
service will be allocated in the new instance. This is a big problem, since those
are services running tasks that need a volume for persisting their data.
Unfortunately, right now is not possible to sync the data between instance, and
as far as I know, is not possible to create services associated to an instance.

In my case when I launched the third instance, my elasticsearch task was relocated
and I had to to run the rake task for indexing the data again.

If you build the project again, and inspect the output, everything should be OK
and the application will be deployed to the cluster.

## A word on Blue-Green deployment

One of the reasons that we had to add one more instance to our cluster, was because
of how ECS launches new updated services. 
ECS uses a style of deployment known as [Blue-Green deployment](http://martinfowler.com/bliki/BlueGreenDeployment.html)
in which the new application is launched without removing the old version. 
This way if something goes wrong with the new version, in ECS words, the ELB
can't reach the health check successfully, you never have downtime, since the old
version will still be there attached to the ELB.
This is great, it gives you more security for launching new releases, but of course
it brings some difficulties.
First, we are running services attached to fixed ports, which means
that you can't run more than one of that service in the same instance, and
since the new and old releases will be running together for some time, you'll always
going to need an instance with that port free. So for example, if you have only one application
that runs in the port 80, you'll need at least `n+1` instances running, with `n` being the number
of tasks that are running the application.
In our case we are using 3 instances not because of this, but because of the resources
that we need (memory, cpu units). Remember that we have 5 services running, and some of them
use a big amount of resources.

## Adding a GitHub hook

The only thing that need to complete the pipeline, is to add a service hook
so we can launch deploys when pushing to some branch that we want. This way
the deploy will be completely automatized.

This is pretty easy, first, go to your GitHub project repository. Then go to
settings, and select  Webhook & Services.

![GitHub Hook](images/github-webhook.png)

Search for the Jenkins (GitHub plugin) and add the following line
replacing the host with your jenkins host:

    http://[JENKINS_HOST]/github-webhook/

And click add service.

Now every time you push a change to the master branch, a build will be launched
for that project in Jenkins.
